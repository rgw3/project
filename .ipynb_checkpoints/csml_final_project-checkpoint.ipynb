{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd85129-5d96-4c39-8599-8a39bcf1f022",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c3fff-77e3-40d8-b1a1-6d4a739b0413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this retrieves my username(s) and api key(s) another user will need to retrieve their own\n",
    "%run project_api_keys.ipynb\n",
    "kaggle_username = os.environ.get(\"KAGGLE_USERNAME\")\n",
    "kaggle_api_key = os.environ.get(\"KAGGLE_API_KEY\")\n",
    "bea_api_key = os.environ.get(\"BEA_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4efef7-9766-4ea4-a224-fc0e5c401f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bea_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b279910-41a1-4681-9edb-726b4d058ae3",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c370c-84be-45ab-b5b8-0d1a81a1b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs - assumes only jupyter-lab has been installed\n",
    "!pip3 install -qU pandas kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6975c7-29d1-4999-bcd7-0822b315631d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a619ec-ccc8-4f20-be29-75ae19547508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3787a-58d8-4d60-91b1-88c35d63a546",
   "metadata": {},
   "source": [
    "# Project Name: Economic Resilience After Natural Disasters\n",
    " - Student <b>Name: Robert Williams</b>\n",
    " - UTeid: <b>rgw65</b>\n",
    " - Course: <b>Case Studies in Machine Learning AI391M (54340)</b>\n",
    " - Term: <b>Fall 2025</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d4de5-71b5-4040-854f-cf1702a262e5",
   "metadata": {},
   "source": [
    "I would like to take a moment to acknowledge <b>[Aurélien Géron](https://www.oreilly.com/pub/au/7106)</b> author of <b>[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)</b>.\n",
    "The structure of this machine learning project is based upon his Machine Learning Project Checklist (Appendix A). It has been an invaluable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8109e-5f14-4bd8-b020-75e62769ca54",
   "metadata": {},
   "source": [
    "## 1. Frame the Problem and Look at the Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefaef2-c058-4966-8532-5cb899826018",
   "metadata": {},
   "source": [
    "## 2. Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e5273-d7bc-40b0-96b8-f8cc1c45f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set data directories\n",
    "OUT_DIR = \"data/raw/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36c02c-a3cc-4d98-be44-a0338afbf0fd",
   "metadata": {},
   "source": [
    "### FEMA Disaster Declarations Summaries (OpenFEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958d755b-9ad9-4e80-93f4-c29aa9130006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for FEMA\n",
    "BASE_URL = \"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries\" # OpenFEMA v2 endpoint\n",
    "PAGE_SIZE = 1000 # OpenFEMA returns up to 1000 per page\n",
    "SLEEP_SEC = 0.2\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "timestamp = dt.datetime.now(dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "OUT_PARQUET = os.path.join(OUT_DIR, f\"disaster_declarations_summaries_{timestamp}.parquet\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, f\"disaster_declarations_summaries_{timestamp}.csv\")\n",
    "\n",
    "# Set to None to pull everything.\n",
    "FILTERS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcdc46a-ed06-496f-9187-ca330eb823fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create filter string\n",
    "def build_filter_str(filters: dict | None) -> str | None:\n",
    "    if not filters:\n",
    "        return None\n",
    "    parts = []\n",
    "    for k, v in filters.items():\n",
    "        # escape single quotes in value\n",
    "        v_escaped = str(v).replace(\"'\", \"''\")\n",
    "        parts.append(f\"{k} eq '{v_escaped}'\")\n",
    "    return \" and \".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3707f6f1-3bed-4852-8e9f-feb3c60938ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch all pages\n",
    "def fetch_openfema_all(base_url: str, page_size: int = 1000, filters: dict | None = None, sleep_sec: float = 0.2) -> pd.DataFrame:\n",
    "    session = requests.Session()\n",
    "    records = []\n",
    "    skip = 0\n",
    "\n",
    "    params = {\n",
    "        \"$top\": page_size,\n",
    "        # You can also use $select to reduce columns if needed, e.g. \"$select\": \"disasterNumber,state,incidentType,...\" \n",
    "    }\n",
    "    filt = build_filter_str(filters)\n",
    "    if filt:\n",
    "        params[\"$filter\"] = filt\n",
    "\n",
    "    while True:\n",
    "        params[\"$skip\"] = skip\n",
    "        r = session.get(base_url, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "\n",
    "        # OpenFEMA returns a \"DisasterDeclarationsSummaries\" array in v2\n",
    "        # If structure changes, print(data.keys()) to inspect.\n",
    "        page = data.get(\"DisasterDeclarationsSummaries\", [])\n",
    "        if not page:\n",
    "            break\n",
    "\n",
    "        records.extend(page)\n",
    "        skip += page_size\n",
    "        time.sleep(sleep_sec)\n",
    "\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f13e5-9de6-4bfc-b050-90654d7edd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data to dataframe\n",
    "df_femaddc = fetch_openfema_all(BASE_URL, page_size=PAGE_SIZE, filters=FILTERS, sleep_sec=SLEEP_SEC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44845afd-a7dd-4dda-9345-2530ec39dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save locally\n",
    "wrote = None\n",
    "try:\n",
    "    df_femaddc.to_parquet(OUT_PARQUET, index=False)\n",
    "    wrote = OUT_PARQUET\n",
    "except Exception as e:\n",
    "    # Fallback if pyarrow/fastparquet isn’t installed:\n",
    "    df_femaddc.to_csv(OUT_CSV, index=False)\n",
    "    wrote = OUT_CSV\n",
    "\n",
    "print(f\"Fetched {len(df_femaddc):,} rows. Saved to: {wrote}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfcf9d2-ca32-4da7-ae65-ba2920a7bfeb",
   "metadata": {},
   "source": [
    "### NOAA Billion-Dollar Weather and Climate Disasters (NCEI) via Kaggle Mirror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd719f-8958-4c4a-8c97-1480f832f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle config\n",
    "KAGGLE_URL = \"https://www.kaggle.com/datasets/landfallmotto/billiondollar-weather-and-climate-disasters/data\"\n",
    "try:\n",
    "    OUT_DIR  # noqa: F821\n",
    "except NameError:\n",
    "    OUT_DIR = \"data/raw/\"\n",
    "\n",
    "BASE_NAME = \"noaa_billion_dollar_disasters\"  # file stem for saved outputs\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ba4e0-9ca1-4eaf-a7d6-d443f36f0d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to authenticate Kaggle (from env vars)\n",
    "def init_kaggle_api(kaggle_username: str | None = None, kaggle_api_key: str | None = None):\n",
    "    user = kaggle_username or os.environ.get(\"KAGGLE_USERNAME\")\n",
    "    # Kaggle lib reads KAGGLE_KEY (preferred) but many users set KAGGLE_API_KEY; accept either.\n",
    "    key  = kaggle_api_key or os.environ.get(\"KAGGLE_KEY\") or os.environ.get(\"KAGGLE_API_KEY\")\n",
    "    if not user or not key:\n",
    "        raise SystemExit(\"Missing Kaggle credentials. Ensure KAGGLE_USERNAME and KAGGLE_KEY (or KAGGLE_API_KEY) are set.\")\n",
    "    os.environ[\"KAGGLE_USERNAME\"] = user\n",
    "    os.environ[\"KAGGLE_KEY\"] = key\n",
    "    try:\n",
    "        from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    except Exception as e:\n",
    "        raise SystemExit(f\"Kaggle package not installed or not importable. Install with `pip install kaggle`. Details: {e}\")\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    print(f\"Kaggle API authenticated as: {user}\")\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867dfe13-a081-4a07-b405-15cfbcc97fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kaggle_api object\n",
    "kaggle_api = init_kaggle_api(\n",
    "    os.environ.get(\"KAGGLE_USERNAME\"),\n",
    "    os.environ.get(\"KAGGLE_KEY\") or os.environ.get(\"KAGGLE_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa11ae7-4901-4f6e-a500-36682731db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse dataset reference from URL\n",
    "def get_kaggle_dataset_ref(url: str) -> str:\n",
    "    m = re.search(r\"/datasets/([^/]+/[^/]+)\", url)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse Kaggle dataset reference from URL: {url}\")\n",
    "    return m.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4216288-2b79-45ec-8be0-63a5809b1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data reference set\n",
    "dataset_ref = get_kaggle_dataset_ref(KAGGLE_URL)\n",
    "print(\"Dataset ref:\", dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d88c42c-5fca-4803-94be-1a5e024f9e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download and unzip kaggle data\n",
    "def kaggle_download_dataset(kaggle_api, dataset_ref: str, base_dir: str, subfolder: str = \"noaa_billion_dollar_disasters\", force: bool = False) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Downloads and unzips a Kaggle dataset into a unique subfolder under base_dir.\n",
    "    Returns a list of extracted file paths.\n",
    "    \"\"\"\n",
    "    # Create a dedicated NOAA folder under your base_dir\n",
    "    out_path = Path(base_dir) / subfolder\n",
    "    out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    existing = [p for p in out_path.rglob(\"*\") if p.is_file()]\n",
    "    if existing and not force:\n",
    "        print(f\"Files already exist under {out_path}. Skipping download (use force=True to re-download).\")\n",
    "        return existing\n",
    "\n",
    "    print(f\"Downloading '{dataset_ref}' to {out_path} ...\")\n",
    "    kaggle_api.dataset_download_files(dataset_ref, path=str(out_path), unzip=True, quiet=False)\n",
    "\n",
    "    extracted = [p for p in out_path.rglob(\"*\") if p.is_file()]\n",
    "    if not extracted:\n",
    "        raise RuntimeError(f\"No files found after download for {dataset_ref} in {out_path}\")\n",
    "\n",
    "    print(f\"Download complete. Found {len(extracted)} files in {out_path}.\")\n",
    "    for p in sorted(extracted)[:10]:\n",
    "        print(\" -\", p.relative_to(out_path))\n",
    "    if len(extracted) > 10:\n",
    "        print(f\" ... (+{len(extracted)-10} more)\")\n",
    "\n",
    "    return extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4ee885-80e9-4e8f-a344-b245ef469df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "downloaded_files = kaggle_download_dataset(kaggle_api, dataset_ref, base_dir=\"data/raw\", subfolder=\"noaa_billion_dollar_disasters\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d489724-7b9c-401d-b94f-c3ce9fb2e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to creat dataframe\n",
    "NOAA_DIR = Path(\"data/raw/noaa_billion_dollar_disasters\")  # matches your unique subfolder\n",
    "BASE_NAME = \"noaa_billion_dollar_disasters\"\n",
    "\n",
    "def select_best_csv(root_dir: Path) -> Path:\n",
    "    csvs = list(root_dir.rglob(\"*.csv\"))\n",
    "    if not csvs:\n",
    "        raise FileNotFoundError(f\"No CSV files found under {root_dir.resolve()}\")\n",
    "    patterns = [\"event\", \"disaster\", \"billion\"]\n",
    "    def score(p: Path):\n",
    "        name = p.name.lower()\n",
    "        hits = sum(bool(re.search(pat, name)) for pat in patterns)\n",
    "        return (hits, p.stat().st_size)\n",
    "    return sorted(csvs, key=score, reverse=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986189c-5aee-46d8-a593-798db4a73422",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_csv = select_best_csv(NOAA_DIR)\n",
    "print(\"Selected CSV:\", selected_csv)\n",
    "\n",
    "# load dataframe\n",
    "df_noaabdd = pd.read_csv(selected_csv, low_memory=False)\n",
    "print(f\"Loaded df_noaabdd: {df_noaabdd.shape[0]:,} rows × {df_noaabdd.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f667b-605d-476a-9e6c-f09e99d56990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save standardized copies in the same NOAA folder\n",
    "ts = dt.datetime.now(dt.UTC).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "out_parquet_ts     = NOAA_DIR / f\"{BASE_NAME}_{ts}.parquet\"\n",
    "out_csv_ts         = NOAA_DIR / f\"{BASE_NAME}_{ts}.csv\"\n",
    "out_parquet_latest = NOAA_DIR / f\"{BASE_NAME}_latest.parquet\"\n",
    "out_csv_latest     = NOAA_DIR / f\"{BASE_NAME}_latest.csv\"\n",
    "\n",
    "try:\n",
    "    df_noaabdd.to_parquet(out_parquet_ts, index=False)\n",
    "    df_noaabdd.to_parquet(out_parquet_latest, index=False)\n",
    "    print(\"Saved timestamped file to:\", out_parquet_ts)\n",
    "    print(\"Saved latest file to     :\", out_parquet_latest)\n",
    "except Exception:\n",
    "    df_noaabdd.to_csv(out_csv_ts, index=False)\n",
    "    df_noaabdd.to_csv(out_csv_latest, index=False)\n",
    "    print(\"Saved timestamped file to:\", out_csv_ts)\n",
    "    print(\"Saved latest file to     :\", out_csv_latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b89dc85-022c-41b3-9420-b3d13a7b47c7",
   "metadata": {},
   "source": [
    "### BEA Regional GDP by County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9df363-da4e-4e38-a0f8-014ee3d7f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEA_BASE_URL = \"https://apps.bea.gov/api/data\"\n",
    "\n",
    "def init_bea_api():\n",
    "    \"\"\"\n",
    "    Reads your BEA API key from env and returns it.\n",
    "    Expected env var: BEA_API_KEY (fallback: BEA_KEY).\n",
    "    \"\"\"\n",
    "    key = os.environ.get(\"BEA_API_KEY\") or os.environ.get(\"BEA_KEY\")\n",
    "    if not key:\n",
    "        raise SystemExit(\n",
    "            \"Missing BEA API key. Set BEA_API_KEY (or BEA_KEY) in your environment \"\n",
    "            \"before continuing.\"\n",
    "        )\n",
    "    print(\"BEA API key found.\")\n",
    "    return key\n",
    "\n",
    "def bea_get(params: dict, timeout: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Thin wrapper around BEA's API.\n",
    "    Adds required fields, handles errors, and returns parsed JSON.\n",
    "    \"\"\"\n",
    "    # required fields (case-insensitive on BEA side, but we’ll keep consistent)\n",
    "    base = {\n",
    "        \"UserID\": params.pop(\"UserID\"),\n",
    "        \"method\": params.pop(\"method\"),\n",
    "        \"datasetname\": params.pop(\"datasetname\"),\n",
    "        \"ResultFormat\": params.pop(\"ResultFormat\", \"JSON\"),\n",
    "    }\n",
    "    query = {**base, **params}\n",
    "\n",
    "    r = requests.get(BEA_BASE_URL, params=query, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    # basic sanity checks BEA-style\n",
    "    if \"BEAAPI\" not in data or \"Results\" not in data[\"BEAAPI\"]:\n",
    "        raise RuntimeError(f\"Unexpected BEA response structure: {list(data.keys())}\")\n",
    "    return data[\"BEAAPI\"][\"Results\"]\n",
    "\n",
    "# Initialize (reads key from your env)\n",
    "BEA_API_KEY = init_bea_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cec7a-efe7-4914-ae0d-162f142f0419",
   "metadata": {},
   "source": [
    "### BLS Local Area Unemployment Statistics (LAUS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967e5ee8-3e5b-4ee5-9cdf-df8467d3c7df",
   "metadata": {},
   "source": [
    "### U.S. Census American Community Survey (ACS) 5-Year Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6b34a-9595-44f5-9429-4b3e8a3bc2e1",
   "metadata": {},
   "source": [
    "### USDA ERS County Typology Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb5a69f-21ea-411d-aa90-5c8c77c1337f",
   "metadata": {},
   "source": [
    "### Census County Business Patterns (CBP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb624cce-b098-49dc-9ad1-499a0bdc9423",
   "metadata": {},
   "source": [
    "### HUD Aggregated USPS Vacancies Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba28306a-854a-40ca-86bd-c707025107d1",
   "metadata": {},
   "source": [
    "### National Risk Index (FEMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f2c2c-d6d0-4ac6-8a78-e24adecfde8b",
   "metadata": {},
   "source": [
    "## 3. Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34acdf-09c2-4f56-afd3-cba6f6131f97",
   "metadata": {},
   "source": [
    "## 4. Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016e529-c33a-42b0-bd96-eeb8a0938911",
   "metadata": {},
   "source": [
    "## 5. Shortlist Promising Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea22d9-3adb-447e-a9a3-fe33bff8e998",
   "metadata": {},
   "source": [
    "## 6. Fine-Tune the System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8860e43-93d6-4c87-bda3-ee6b2f5d0a6a",
   "metadata": {},
   "source": [
    "## 7. Present your Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9235995-251c-4c2e-91c5-8bbd9b97b627",
   "metadata": {},
   "source": [
    "## 8. Launch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751ed45-81ab-43f2-adc1-b17534672e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
