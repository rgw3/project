{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd85129-5d96-4c39-8599-8a39bcf1f022",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f184f7-35e4-4e59-91e6-9b91bf39b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old data this can be deleted when published, for using when Kernel must restart\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Construct the path relative to the notebook location\n",
    "data_dir = os.path.join(os.path.dirname(os.getcwd()), 'data', 'raw')\n",
    "\n",
    "# Confirm the path\n",
    "print(f\"Target directory: {data_dir}\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(data_dir):\n",
    "    # Iterate through everything in the folder\n",
    "    for filename in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        try:\n",
    "            # Remove files and directories\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.remove(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "    print(\"All data in /data/raw has been deleted.\")\n",
    "else:\n",
    "    print(\"The /data/raw directory does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad285ea9-491c-4897-8668-35e9c33dc5b5",
   "metadata": {},
   "source": [
    "### Import API Keys\n",
    "This imports my API keys, users will need to set up their own API keys in a file called <b><i>project_api_keys.ipynb</i></b> with the following code:<br><br>\n",
    "<b>\n",
    "import os<br>\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"your_kaggle_user_name\"<br>\n",
    "os.environ[\"KAGGLE_API_KEY\"] = \"your_kaggle_api_key\"<br>\n",
    "os.environ[\"BEA_API_KEY\"] = \"your_bea_api_key\"<br></b>\n",
    "\n",
    "<i>Alternatively, users can simply create their own variables with values without using an alternative file.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea69334-d6e4-4a61-be15-cf71459be9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run project_api_keys.ipynb\n",
    "kaggle_username = os.environ.get(\"KAGGLE_USERNAME\")\n",
    "kaggle_api_key = os.environ.get(\"KAGGLE_API_KEY\")\n",
    "bea_api_key = os.environ.get(\"BEA_API_KEY\")\n",
    "\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_username\n",
    "os.environ['KAGGLE_KEY'] = kaggle_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b279910-41a1-4681-9edb-726b4d058ae3",
   "metadata": {},
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c370c-84be-45ab-b5b8-0d1a81a1b294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# installs - assumes only jupyter-lab has been installed\n",
    "!pip3 install -qU pandas kaggle matplotlib scikit-learn lightgbm xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6975c7-29d1-4999-bcd7-0822b315631d",
   "metadata": {},
   "source": [
    "## Imports\n",
    "This is done at the start of the notebook as opposed to inline so that all installs and libraries are successful and that the focus can be on the ML process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a619ec-ccc8-4f20-be29-75ae19547508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "import kaggle\n",
    "import matplotlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b3787a-58d8-4d60-91b1-88c35d63a546",
   "metadata": {},
   "source": [
    "# Project Name: Post Disaster Economic Recovery Model\n",
    " - Student <b>Name: Robert Williams</b>\n",
    " - UTeid: <b>rgw65</b>\n",
    " - Course: <b>Case Studies in Machine Learning AI391M (54340)</b>\n",
    " - Term: <b>Fall 2025</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4d4de5-71b5-4040-854f-cf1702a262e5",
   "metadata": {},
   "source": [
    "I would like to take a moment to acknowledge <b>[Aurélien Géron](https://www.oreilly.com/pub/au/7106)</b> author of <b>[Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)</b>.\n",
    "The structure of this machine learning project is based upon his Machine Learning Project Checklist (Appendix A). It has been an invaluable resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c8109e-5f14-4bd8-b020-75e62769ca54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Frame the Problem and Look at the Big Picture\n",
    " - Define the objective in business terms.\n",
    " - How will your solution be used?\n",
    " - What are the current solutions/workarounds (if any)?\n",
    " - How should you frame this problem (supervised/unsupervised, online/offline, etc.)?\n",
    " - How should performance be measured?\n",
    " - Is the performance measure aligned with the business objective?\n",
    " - What would be the minimum performance needed to reach the business objective?\n",
    " - What are comparable problems? Can you reuse experience or tools?\n",
    " - Is human expertise available?\n",
    " - How would you solve the problem manually?\n",
    " - List the assumptions you (or others) have made so far.\n",
    " - Verify assumptions if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afefaef2-c058-4966-8532-5cb899826018",
   "metadata": {},
   "source": [
    "## 2. Get the Data\n",
    "Note: automate as much as possible so you can easily get fresh data.\n",
    " - List the data you need and how much you need.\n",
    " - Find and document where you can get that data.\n",
    " - Check how much space it will take.\n",
    " - Check legal obligations, and get authorization if necessary.\n",
    " - Get access authorizations.\n",
    " - Create a workspace (with enough storage space).\n",
    " - Get the data.\n",
    " - Convert the data to a format you can easily manipulate (without changing the data itself).\n",
    " - Ensure sensitive information is deleted or protected (e.g., anonymized).\n",
    " - Check the size and type of data (time series, sample, geographical, etc.).\n",
    " - Sample a test set, put it aside, and never look at it (no data snooping!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7826ebd-0ac3-42fc-9939-57428513ff12",
   "metadata": {},
   "source": [
    "### Configure folder structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09a7e63-8688-4172-9d87-7a6b5974fc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths relative to the notebooks folder\n",
    "# When running from notebooks folder, go up one level to project root, then into data/raw\n",
    "NOTEBOOK_DIR = Path.cwd()  # Current working directory (notebooks folder)\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent  # Go up to project folder\n",
    "DATA_RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "# Create the data/raw directory if it doesn't exist\n",
    "DATA_RAW_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a844c-4b0c-42a3-b2fd-2f58da1809c0",
   "metadata": {},
   "source": [
    "### FEMA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ba462-e486-41f3-aa8f-cc838647eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEMA OpenFEMA API endpoint for Disaster Declarations Summaries\n",
    "FEMA_URL = \"https://www.fema.gov/api/open/v2/DisasterDeclarationsSummaries.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58875abf-a364-4ebd-b469-ecfba5632ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File to save the downloaded data\n",
    "FEMA_FILE = DATA_RAW_DIR / \"fema_disaster_declarations.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eabcae-48e4-4da3-b53f-ee27b75d5cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download fema data\n",
    "def download_fema_data():\n",
    "    \"\"\"\n",
    "    Download FEMA Disaster Declarations Summaries dataset\n",
    "    \"\"\"\n",
    "    print(\"Downloading FEMA data...\")\n",
    "    print(f\"URL: {FEMA_URL}\")\n",
    "    \n",
    "    try:\n",
    "        # Download the file\n",
    "        response = requests.get(FEMA_URL, timeout=60)\n",
    "        response.raise_for_status()  # Raise an error for bad status codes\n",
    "        \n",
    "        # Save to file\n",
    "        with open(FEMA_FILE, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"✓ Data downloaded successfully: {FEMA_FILE}\")\n",
    "        print(f\"  File size: {FEMA_FILE.stat().st_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"✗ Error downloading data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba422eb4-b13c-4ba3-9865-cc24986a4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fema data to dataframe\n",
    "def load_fema_data():\n",
    "    \"\"\"\n",
    "    Load FEMA data into a pandas DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading FEMA data into DataFrame...\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV into DataFrame\n",
    "        df = pd.read_csv(FEMA_FILE, low_memory=False)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b425f-6871-46c3-a3e8-c12a7e0bf69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "download_fema_data()\n",
    "# load data into dataframe\n",
    "fema_df = load_fema_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3052252-e96c-48f7-a850-3e2c7b895096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check on data\n",
    "fema_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f546dd-5f62-40c4-aae4-c906963cb4f6",
   "metadata": {},
   "source": [
    "### NOAA Billion-Dollar Weather and Climate Disasters\n",
    "### This section needs help as it is not pulling the right data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab77c36-f4b1-4ea6-b071-0b1d2541b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA Billion-Dollar Disasters data\n",
    "# Note: This dataset is available on Kaggle\n",
    "# Direct download link from NOAA NCEI\n",
    "NOAA_URL = \"https://www.ncei.noaa.gov/pub/data/billions/events.csv\"\n",
    "NOAA_FILE = DATA_RAW_DIR / \"events-US-1980-2021.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c2d07-f286-4b4f-9648-7c342903ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download NOAA data from Kaggle\n",
    "def download_noaa_data():\n",
    "    \"\"\"\n",
    "    Download NOAA Billion-Dollar Weather and Climate Disasters dataset from Kaggle\n",
    "    Note: Uses KAGGLE_USERNAME and KAGGLE_API_KEY from environment variables\n",
    "    Dataset: https://www.kaggle.com/datasets/noaa/noaa-billion-dollar-weather-and-climate-disasters\n",
    "    \"\"\"\n",
    "    print(\"Downloading NOAA data from Kaggle...\")\n",
    "    \n",
    "    try:\n",
    "        # Kaggle dataset identifier\n",
    "        dataset = \"noaa/noaa-billion-dollar-weather-and-climate-disasters\"\n",
    "        dataset = \"christinezinkand/us-billiondollar-weather-and-climate-disasters\"\n",
    "        \n",
    "        # Download dataset to the raw data directory\n",
    "        kaggle.api.dataset_download_files(\n",
    "            dataset,\n",
    "            path=DATA_RAW_DIR,\n",
    "            unzip=True,\n",
    "            quiet=False\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Data downloaded successfully to: {DATA_RAW_DIR}\")\n",
    "        \n",
    "        # The main file should be in the directory\n",
    "        # Check what files were downloaded\n",
    "        downloaded_files = list(DATA_RAW_DIR.glob(\"*.csv\"))\n",
    "        print(f\"  Downloaded files: {[f.name for f in downloaded_files]}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading data: {e}\")\n",
    "        print(\"\\nMake sure you have:\")\n",
    "        print(\"1. Valid Kaggle API credentials in your project_api_keys.ipynb\")\n",
    "        print(\"2. Accepted the dataset terms at: https://www.kaggle.com/datasets/noaa/noaa-billion-dollar-weather-and-climate-disasters\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ea6bea-99bc-4589-b086-0731ec0ca0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load NOAA data to dataframe\n",
    "def load_noaa_data():\n",
    "    \"\"\"\n",
    "    Load NOAA data into a pandas DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading NOAA data into DataFrame...\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV into DataFrame\n",
    "        df = pd.read_csv(NOAA_FILE, low_memory=False)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa6d91-89d7-4d3b-b48c-67b7c4fd8c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "download_noaa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4af8a-858d-44f2-baa6-7896022a1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into dataframe\n",
    "noaa_df = load_noaa_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5432bf-dc04-4521-8d12-a42017282296",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spot check on data\n",
    "noaa_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b862a-cbdd-4e4e-b351-9c6297341cb7",
   "metadata": {},
   "source": [
    "### BEA Regional GDP by County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc569be-b40c-461b-84e3-da7d7ee4bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEA API endpoint for Regional GDP by County\n",
    "BEA_API_URL = \"https://apps.bea.gov/api/data\"\n",
    "BEA_FILE = DATA_RAW_DIR / \"bea_county_gdp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36c4a65-dee2-436b-bb12-9581b34e6836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download BEA Regional GDP data\n",
    "def download_bea_data():\n",
    "    \"\"\"\n",
    "    Download BEA Regional GDP by County (CAGDP) dataset using BEA API\n",
    "    Note: Uses BEA_API_KEY from environment variables\n",
    "    API Documentation: https://apps.bea.gov/api/\n",
    "    \"\"\"\n",
    "    print(\"Downloading BEA Regional GDP data...\")\n",
    "    \n",
    "    try:\n",
    "        # BEA API parameters for Regional GDP by County\n",
    "        params = {\n",
    "            'UserID': bea_api_key,\n",
    "            'method': 'GetData',\n",
    "            'datasetname': 'Regional',\n",
    "            'TableName': 'CAGDP9',  # GDP by County, Metro, and Other Areas\n",
    "            'LineCode': '1',  # All industry total\n",
    "            'GeoFips': 'COUNTY',  # All counties\n",
    "            'Year': 'ALL',  # All available years\n",
    "            'ResultFormat': 'JSON'\n",
    "        }\n",
    "        \n",
    "        print(f\"API URL: {BEA_API_URL}\")\n",
    "        print(\"Fetching data (this may take a minute)...\")\n",
    "        \n",
    "        # Make API request\n",
    "        response = requests.get(BEA_API_URL, params=params, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Check for API errors\n",
    "        if 'BEAAPI' in data and 'Error' in data['BEAAPI']:\n",
    "            error_msg = data['BEAAPI']['Error']['ErrorDetail']\n",
    "            print(f\"✗ BEA API Error: {error_msg}\")\n",
    "            return False\n",
    "        \n",
    "        # Extract data from JSON\n",
    "        if 'BEAAPI' in data and 'Results' in data['BEAAPI']:\n",
    "            results = data['BEAAPI']['Results']['Data']\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(results)\n",
    "            \n",
    "            # Save to CSV\n",
    "            df.to_csv(BEA_FILE, index=False)\n",
    "            \n",
    "            print(f\"✓ Data downloaded successfully: {BEA_FILE}\")\n",
    "            print(f\"  File size: {BEA_FILE.stat().st_size / (1024*1024):.2f} MB\")\n",
    "            print(f\"  Records: {len(df):,}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"✗ Unexpected API response format\")\n",
    "            return False\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"✗ Error downloading data: {e}\")\n",
    "        print(\"\\nMake sure you have:\")\n",
    "        print(\"1. Valid BEA API key in your project_api_keys.ipynb\")\n",
    "        print(\"2. Register for a free key at: https://apps.bea.gov/api/signup/\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314ba09-1dd8-4663-909e-8e3e6f181c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BEA data to dataframe\n",
    "def load_bea_data():\n",
    "    \"\"\"\n",
    "    Load BEA data into a pandas DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading BEA data into DataFrame...\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV into DataFrame\n",
    "        df = pd.read_csv(BEA_FILE, low_memory=False)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8bd78-9af8-4964-8974-07a9836996d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute download\n",
    "download_bea_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac9e41-c97d-4bd8-8cef-0ef56ee7687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into dataframe\n",
    "bea_df = load_bea_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e388254-35ef-42f4-b29f-cb49f5caebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "bea_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357af03f-1a54-4090-91c0-f30e1803dd10",
   "metadata": {},
   "source": [
    "### Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bf3c5f-6382-49ba-be5b-3381e955c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# census ACS API endpoint\n",
    "CENSUS_API_URL = \"https://api.census.gov/data\"\n",
    "CENSUS_FILE = DATA_RAW_DIR / \"census_acs_county.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadfee4d-2c1e-457c-829f-33cbfed47040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download Census ACS data\n",
    "def download_census_data():\n",
    "    \"\"\"\n",
    "    Download Census American Community Survey (ACS) 5-Year Estimates\n",
    "    Note: Census API does not require authentication for public data\n",
    "    API Documentation: https://www.census.gov/data/developers/data-sets/acs-5year.html\n",
    "    \n",
    "    This downloads key socioeconomic variables at the county level:\n",
    "    - Median household income\n",
    "    - Poverty rate\n",
    "    - Educational attainment\n",
    "    - Unemployment rate\n",
    "    \"\"\"\n",
    "    print(\"Downloading Census ACS data...\")\n",
    "    \n",
    "    try:\n",
    "        # We'll download the most recent 5-year ACS data (2022)\n",
    "        # Key variables for resilience analysis\n",
    "        variables = [\n",
    "            'B19013_001E',  # Median household income\n",
    "            'B17001_002E',  # Population below poverty level\n",
    "            'B17001_001E',  # Total population for poverty calculation\n",
    "            'B23025_005E',  # Unemployed\n",
    "            'B23025_003E',  # In labor force (for unemployment rate)\n",
    "            'B15003_022E',  # Bachelor's degree\n",
    "            'B15003_023E',  # Master's degree\n",
    "            'B15003_024E',  # Professional degree\n",
    "            'B15003_025E',  # Doctorate degree\n",
    "            'B15003_001E',  # Total population 25+ (for education rate)\n",
    "            'B25003_002E',  # Owner occupied housing\n",
    "            'B25003_001E',  # Total occupied housing units\n",
    "            'NAME'          # Geographic name\n",
    "        ]\n",
    "        \n",
    "        year = 2022\n",
    "        dataset = 'acs/acs5'\n",
    "        \n",
    "        params = {\n",
    "            'get': ','.join(variables),\n",
    "            'for': 'county:*',  # All counties\n",
    "            'key': None  # Census API doesn't require key for public data\n",
    "        }\n",
    "        \n",
    "        url = f\"{CENSUS_API_URL}/{year}/{dataset}\"\n",
    "        print(f\"API URL: {url}\")\n",
    "        print(\"Fetching data (this may take a minute)...\")\n",
    "        \n",
    "        # Make API request\n",
    "        response = requests.get(url, params=params, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Convert to DataFrame (first row is headers)\n",
    "        df = pd.DataFrame(data[1:], columns=data[0])\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(CENSUS_FILE, index=False)\n",
    "        \n",
    "        print(f\"✓ Data downloaded successfully: {CENSUS_FILE}\")\n",
    "        print(f\"  File size: {CENSUS_FILE.stat().st_size / (1024*1024):.2f} MB\")\n",
    "        print(f\"  Records: {len(df):,} counties\")\n",
    "        return True\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"✗ Error downloading data: {e}\")\n",
    "        print(\"\\nNote: Census API is free and doesn't require authentication\")\n",
    "        print(\"Visit: https://www.census.gov/data/developers/data-sets/acs-5year.html\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing data: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a1c86d-8784-4059-88a4-af7101c31752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Census data to dataframe\n",
    "def load_census_data():\n",
    "    \"\"\"\n",
    "    Load Census ACS data into a pandas DataFrame\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading Census ACS data into DataFrame...\")\n",
    "    \n",
    "    try:\n",
    "        # Read CSV into DataFrame\n",
    "        df = pd.read_csv(CENSUS_FILE, low_memory=False)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e82ae3-4bc0-45be-a25c-d4fb7c584960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch census data download\n",
    "download_census_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd33d3dc-2fd6-42c5-b0da-b53740d601ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into dataframe\n",
    "census_df = load_census_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cfb4aa-aa4b-4d11-bb1b-f2b90b5a02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "census_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f2c2c-d6d0-4ac6-8a78-e24adecfde8b",
   "metadata": {},
   "source": [
    "## 3. Explore the Data\n",
    "Note: try to get insights from a field expert for these steps.\n",
    " - Create a copy of the data for exploration (sampling it down to a manageable size if necessary).\n",
    " - Create a Jupyter notebook to keep a record of your data exploration.\n",
    " - Study each attribute and its characteristics:\n",
    "    - Name\n",
    "    - Type (categorical, int/float, bounded/unbounded, text, structured, etc.)\n",
    "    - % of missing values\n",
    "    - Noisiness and type of noise (stochastic, outliers, rounding errors, etc.)\n",
    "    - Usefulness for the task\n",
    "    - Type of distribution (Gaussian, uniform, logarithmic, etc.)\n",
    " - For supervised learning tasks, identify the target attribute(s).\n",
    " - Visualize the data.\n",
    " - Study the correlations between attributes.\n",
    " - Study how you would solve the problem manually.\n",
    " - Identify the promising transformations you may want to apply.\n",
    " - Identify extra data that would be useful (go back to “Get the Data” on page 780).\n",
    " - Document what you have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd60140-d08d-4317-8ab2-79994ee6d104",
   "metadata": {},
   "source": [
    "### FEMA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d3147-e98c-4cbe-84f4-e301a92bac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the FEMA data for exploration\n",
    "fema_explore = fema_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7de23-c62c-4172-b0ce-cf17b80cead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset info\n",
    "fema_explore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90176e28-6f2d-4704-8074-664be4f1825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical summary\n",
    "fema_explore.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b4b545-0c04-4b15-9334-9f831d3be948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types and non-null counts\n",
    "fema_explore.dtypes.to_frame(name='dtype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da9dc0-dbe7-49f4-a55d-afd09f0aa8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values in key categorical columns\n",
    "fema_explore['incidentType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7078fef-2bbe-49e3-b1af-f1be43d1e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaration types\n",
    "fema_explore['declarationType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97128c0-f618-4400-a497-4b6a600e0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of disasters over time\n",
    "fema_explore['fyDeclared'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d243203-a567-4d00-ae59-f31c053ed294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns relevant for modeling economic resilience\n",
    "# Key columns we need:\n",
    "# - fipsStateCode + fipsCountyCode (to create county FIPS for joining with BEA/Census)\n",
    "# - fyDeclared or declarationDate (temporal - disaster year)\n",
    "# - incidentType (disaster type)\n",
    "# - ihProgramDeclared, iaProgramDeclared, paProgramDeclared, hmProgramDeclared (federal assistance programs)\n",
    "# - disasterNumber (unique disaster identifier)\n",
    "\n",
    "relevant_cols = [\n",
    "    'disasterNumber',\n",
    "    'state', \n",
    "    'fipsStateCode',\n",
    "    'fipsCountyCode',\n",
    "    'declarationType',\n",
    "    'declarationDate',\n",
    "    'fyDeclared',\n",
    "    'incidentType',\n",
    "    'incidentBeginDate',\n",
    "    'ihProgramDeclared',\n",
    "    'iaProgramDeclared', \n",
    "    'paProgramDeclared',\n",
    "    'hmProgramDeclared'\n",
    "]\n",
    "\n",
    "fema_relevant = fema_explore[relevant_cols].copy()\n",
    "fema_relevant.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d8959-3b7a-46dd-8da1-66c104313648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disaster frequency over time\n",
    "ax = fema_relevant['fyDeclared'].hist(bins=50, figsize=(10, 6))\n",
    "ax.set_xlabel('Fiscal Year Declared')\n",
    "ax.set_ylabel('Number of Disaster Declarations')\n",
    "ax.set_title('FEMA Disaster Declarations Over Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7a1277-62d6-4738-9662-ba97ffe6fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top incident types\n",
    "ax = fema_relevant['incidentType'].value_counts().head(10).plot(kind='barh', figsize=(10, 6))\n",
    "ax.set_xlabel('Number of Declarations')\n",
    "ax.set_ylabel('Incident Type')\n",
    "ax.set_title('Top 10 FEMA Incident Types')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3129911e-5f62-4b5a-a258-cf55a9d60f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of federal assistance programs\n",
    "assistance_programs = fema_relevant[['ihProgramDeclared', 'iaProgramDeclared', \n",
    "                                      'paProgramDeclared', 'hmProgramDeclared']].sum()\n",
    "assistance_programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffbe1c7-f4d9-45be-b264-1b3b6f71a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check declaration type distribution\n",
    "fema_relevant['declarationType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ead0d-b75d-4cc2-a507-2c684fd8ff06",
   "metadata": {},
   "source": [
    "### NOAA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbf208-7b45-4af1-86ab-bf16460766d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of NOAA data for exploration\n",
    "noaa_explore = noaa_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f430057c-b7c6-4a96-bb2a-45391c9daaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about NOAA dataset\n",
    "noaa_explore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da442f8-d7b6-48fa-bba9-52234ca75353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "noaa_explore.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d0464-df80-401d-93bd-6a63bb92e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows to see the structure\n",
    "noaa_explore.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc06767-f4f1-4d30-b716-96e4ca142ebe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "noaa_explore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8492c60-4167-4f2a-8abe-37700342cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the index\n",
    "noaa_explore.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd40479-6c8a-4697-9b83-f386824b94c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload NOAA data with proper header handling\n",
    "noaa_df = pd.read_csv(DATA_RAW_DIR / \"events-US-1980-2021.csv\", \n",
    "                      skiprows=1)  # Skip the title row\n",
    "noaa_explore = noaa_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e1aa3-1135-464e-bac4-195971b75fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the structure again\n",
    "noaa_explore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27475ada-98fe-45fd-8aa7-b4c5fd4a41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "noaa_explore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad46bf-71b4-429f-880a-589fba92d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "noaa_explore.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223319e0-0816-40ae-b3b9-a590630876d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check disaster types\n",
    "noaa_explore['Disaster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a888f5-0e4b-4ce9-874b-8ec31db32ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the temporal range\n",
    "print(f\"Date range: {noaa_explore['Begin Date'].min()} to {noaa_explore['End Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade5d96-6bd1-45a8-ac24-ac7d2fda8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disaster types\n",
    "ax = noaa_explore['Disaster'].value_counts().plot(kind='barh', figsize=(10, 6))\n",
    "ax.set_xlabel('Number of Events')\n",
    "ax.set_ylabel('Disaster Type')\n",
    "ax.set_title('NOAA Billion-Dollar Disasters by Type (1980-2021)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6485bc-9f65-41cd-b87a-13c06dfdd2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost distribution\n",
    "ax = noaa_explore['Total CPI-Adjusted Cost (Millions of Dollars)'].hist(bins=30, figsize=(10, 6))\n",
    "ax.set_xlabel('Cost (Millions of Dollars)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Billion-Dollar Disaster Costs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47dbaec-5c61-4b9f-bde8-1d9d01b394de",
   "metadata": {},
   "source": [
    "### BEA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea3c6c-4c1c-4d4e-830a-7d1df9193d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of BEA data for exploration\n",
    "bea_explore = bea_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a259c3-68e5-4aa1-aa06-050947a91c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about BEA dataset\n",
    "bea_explore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baea081-50bb-47ac-944f-3877b5dfaa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "bea_explore.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745db1f-21eb-4b09-afdd-4c3ac2d42995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "bea_explore.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc225d38-2c28-4923-8c3a-bc70ced6e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check temporal coverage\n",
    "bea_explore['TimePeriod'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a80c5c3-a58a-461d-96fb-27f726139395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of unique counties\n",
    "print(f\"Number of unique counties: {bea_explore['GeoFips'].nunique()}\")\n",
    "print(f\"Number of years: {bea_explore['TimePeriod'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364aa99-95c2-4574-aca8-63728e2387b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at GDP value distribution\n",
    "ax = bea_explore['DataValue'].hist(bins=50, figsize=(10, 6), log=True)\n",
    "ax.set_xlabel('GDP (Thousands of 2017 Dollars)')\n",
    "ax.set_ylabel('Frequency (log scale)')\n",
    "ax.set_title('Distribution of County GDP Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99646b0-2a3a-485b-a55a-1519ba56f03e",
   "metadata": {},
   "source": [
    "### Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e33d8e-1a6c-4ef4-a3c4-3e346614acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of Census data for exploration\n",
    "census_explore = census_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01415c29-42f9-4e9f-a84d-f896ce6ffce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about Census dataset\n",
    "census_explore.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae945f2d-da12-4bd0-872a-9448c2749b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "census_explore.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eca805-6d9c-41d7-87ab-814fc3d86f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "census_explore.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa910e-761b-4ce6-9a3d-b5be0527f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create readable column names for Census variables\n",
    "census_column_map = {\n",
    "    'B19013_001E': 'median_household_income',\n",
    "    'B17001_002E': 'pop_below_poverty',\n",
    "    'B17001_001E': 'total_pop_poverty_status',\n",
    "    'B23025_005E': 'unemployed',\n",
    "    'B23025_003E': 'in_labor_force',\n",
    "    'B15003_022E': 'bachelors_degree',\n",
    "    'B15003_023E': 'masters_degree',\n",
    "    'B15003_024E': 'professional_degree',\n",
    "    'B15003_025E': 'doctorate_degree',\n",
    "    'B15003_001E': 'total_pop_25plus',\n",
    "    'B25003_002E': 'owner_occupied_housing',\n",
    "    'B25003_001E': 'total_occupied_housing',\n",
    "    'NAME': 'county_name',\n",
    "    'state': 'state_fips',\n",
    "    'county': 'county_fips_part'\n",
    "}\n",
    "\n",
    "# Show what each column represents\n",
    "for code, name in census_column_map.items():\n",
    "    print(f\"{code:15} -> {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a931df-1ba4-4891-a8ce-2120e1b8aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any unusual values (like the negative median income in the summary)\n",
    "census_explore[census_explore['B19013_001E'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00828486-97b0-406b-8250-b1619e98ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize median household income distribution (excluding the special code)\n",
    "ax = census_explore[census_explore['B19013_001E'] > 0]['B19013_001E'].hist(bins=50, figsize=(10, 6))\n",
    "ax.set_xlabel('Median Household Income ($)')\n",
    "ax.set_ylabel('Number of Counties')\n",
    "ax.set_title('Distribution of Median Household Income by County (2022)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a14b0-a575-41dc-a58b-ed81680b15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize poverty rate\n",
    "census_explore['poverty_rate'] = (census_explore['B17001_002E'] / census_explore['B17001_001E']) * 100\n",
    "ax = census_explore['poverty_rate'].hist(bins=50, figsize=(10, 6))\n",
    "ax.set_xlabel('Poverty Rate (%)')\n",
    "ax.set_ylabel('Number of Counties')\n",
    "ax.set_title('Distribution of Poverty Rates by County (2022)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a8d80f-5d7b-4676-88f7-34ac758bbea4",
   "metadata": {},
   "source": [
    "### Key Findings from Data Exploration\n",
    "\n",
    "#### FEMA Disaster Declarations (68,509 records)\n",
    "- **Temporal Coverage**: 1953-2026, with significant increase in recent years\n",
    "- **Top Incident Types**: Severe Storm (19,287), Hurricane (13,721), Flood (11,207)\n",
    "- **Declaration Types**: DR (46,006), EM (20,433), FM (2,070)\n",
    "- **Federal Assistance**: PA programs most common (64,026), followed by HM (30,021) and IA (17,187)\n",
    "- **Key Columns for Modeling**: fipsStateCode, fipsCountyCode, fyDeclared, incidentType, assistance programs\n",
    "- **Data Quality**: Missing values in lastIAFilingDate (71.6%), designatedIncidentTypes (69.8%), disasterCloseoutDate (24%)\n",
    "\n",
    "#### NOAA Billion-Dollar Disasters (323 events, 1980-2021)\n",
    "- **Disaster Types**: Severe Storm (152), Tropical Cyclone (57), Flooding (36), Drought (29)\n",
    "- **Cost Range**: $1.0B - $180.0B (CPI-adjusted)\n",
    "- **Limitation**: Multi-state events, needs disaggregation to county level\n",
    "- **Use**: Disaster intensity/severity measure\n",
    "\n",
    "#### BEA Regional GDP (71,714 county-year observations)\n",
    "- **Counties**: 3,118 counties\n",
    "- **Years**: 2001-2023 (23 years, complete panel)\n",
    "- **Key Variable**: DataValue (GDP in thousands of 2017 dollars)\n",
    "- **Use**: Calculate recovery rate (target variable)\n",
    "\n",
    "#### Census ACS 2022 (3,222 counties)\n",
    "- **Socioeconomic Variables**: Income, poverty, unemployment, education, housing\n",
    "- **Data Quality Issue**: Loving County, TX has suppressed values (-666666666)\n",
    "- **Use**: Baseline vulnerability and capacity indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34acdf-09c2-4f56-afd3-cba6f6131f97",
   "metadata": {},
   "source": [
    "## 4. Prepare the Data\n",
    "Notes:\n",
    " - Work on copies of the data (keep the original dataset intact).\n",
    " - Write functions for all data transformations you apply, for five reasons:\n",
    "    - So you can easily prepare the data the next time you get a fresh dataset\n",
    "    - So you can apply these transformations in future projects\n",
    "    - To clean and prepare the test set\n",
    "    - To clean and prepare new data instances once your solution is live\n",
    "    - To make it easy to treat your preparation choices as hyperparameters\n",
    "1. Clean the data:\n",
    "    - Fix or remove outliers (optional).\n",
    "    - Fill in missing values (e.g., with zero, mean, median…) or drop their rows (or columns).\n",
    "2. Perform feature selection (optional):\n",
    "    - Drop the attributes that provide no useful information for the task.\n",
    "3. Perform feature engineering, where appropriate:\n",
    "    - Discretize continuous features.\n",
    "    - Decompose features (e.g., categorical, date/time, etc.).\n",
    "    - Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.).\n",
    "    - Aggregate features into promising new features.\n",
    "4. Perform feature scaling:\n",
    "    - Standardize or normalize features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2de4d8d-8fcb-44dc-a9e1-4c47bbf4859a",
   "metadata": {},
   "source": [
    "#### FEMA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ef4aea-dc57-4102-9d10-6a737c075f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of FEMA data for prep\n",
    "fema_prep = fema_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0468bc4-0b02-42c1-aeca-879fa495485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape before prep\n",
    "fema_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cde57d-28dc-48fb-89f9-69beaea53f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-digit county FIPS code (2-digit state + 3-digit county)\n",
    "fema_prep['county_fips'] = (fema_prep['fipsStateCode'].astype(str).str.zfill(2) + \n",
    "                             fema_prep['fipsCountyCode'].astype(str).str.zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd616f39-d1ea-45e1-8ba7-02b771e899c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the result\n",
    "fema_prep[['fipsStateCode', 'fipsCountyCode', 'county_fips']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bb0fc9-a635-4b7a-b9d6-471f87cb0a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date columns to datetime\n",
    "fema_prep['declarationDate'] = pd.to_datetime(fema_prep['declarationDate'])\n",
    "fema_prep['incidentBeginDate'] = pd.to_datetime(fema_prep['incidentBeginDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11b468a-d4a5-4507-969f-54c44ae44a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate the conversion\n",
    "fema_prep[['declarationDate', 'incidentBeginDate', 'fyDeclared']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45803bb9-5335-4009-b4c5-b988df432a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select relevant columns for modeling\n",
    "fema_cols = [\n",
    "    'disasterNumber',\n",
    "    'county_fips',\n",
    "    'state',\n",
    "    'declarationType',\n",
    "    'declarationDate',\n",
    "    'fyDeclared',\n",
    "    'incidentType',\n",
    "    'incidentBeginDate',\n",
    "    'ihProgramDeclared',\n",
    "    'iaProgramDeclared',\n",
    "    'paProgramDeclared',\n",
    "    'hmProgramDeclared'\n",
    "]\n",
    "\n",
    "fema_prep = fema_prep[fema_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5f38a-21db-48ce-af06-b1e97b59a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "fema_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63a3a60-fa41-4e5f-bdc9-8880408681c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for missing values in prepared FEMA data\n",
    "fema_prep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a559fb8-8f57-42db-a85d-4a4ad3d06cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate FEMA data to county-year level\n",
    "# Count disasters by type, sum assistance programs\n",
    "fema_county_year = fema_prep.groupby(['county_fips', 'fyDeclared']).agg({\n",
    "    'disasterNumber': 'count',  # Total number of disasters\n",
    "    'ihProgramDeclared': 'sum',\n",
    "    'iaProgramDeclared': 'sum',\n",
    "    'paProgramDeclared': 'sum',\n",
    "    'hmProgramDeclared': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "fema_county_year.columns = ['county_fips', 'year', 'disaster_count', \n",
    "                             'ih_program_total', 'ia_program_total', \n",
    "                             'pa_program_total', 'hm_program_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33f30f3-ddd9-4d95-a6a3-bc7ad66acd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "fema_county_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6eee4-1df1-44e9-b5a6-893e78f8136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create incident type counts by county-year\n",
    "incident_types = fema_prep.groupby(['county_fips', 'fyDeclared', 'incidentType']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e98677-aabd-405e-a480-1609d06889fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to create separate columns for each incident type\n",
    "incident_pivot = incident_types.pivot_table(\n",
    "    index=['county_fips', 'fyDeclared'], \n",
    "    columns='incidentType', \n",
    "    values='count', \n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "incident_pivot.columns.name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e63cf-2532-4763-b90d-45c93b1aafbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "incident_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5f6cb-34c5-4767-b1ea-5c0ac910bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the aggregated counts with incident types\n",
    "fema_final = fema_county_year.merge(\n",
    "    incident_pivot, \n",
    "    left_on=['county_fips', 'year'], \n",
    "    right_on=['county_fips', 'fyDeclared'],\n",
    "    how='left'\n",
    ").drop(columns=['fyDeclared'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae1c80-1b00-480d-bed0-64afed01371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "fema_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb83f6-d3fa-46e7-b889-183947302558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "fema_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ce42af-b4df-4a6f-b554-c7485c9ad19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prepared FEMA data\n",
    "fema_final.to_csv(DATA_RAW_DIR.parent / \"processed\" / \"fema_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256520ef-fb46-4882-982e-a2a64b4f2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm save\n",
    "print(f\"FEMA data prepared: {fema_final.shape[0]:,} county-year observations with {fema_final.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f805792-8dfb-4226-b88f-0b9a0838754e",
   "metadata": {},
   "source": [
    "#### NOAA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097de304-c32f-4af8-803e-05fec1ddc1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a working copy of NOAA data for prep\n",
    "noaa_prep = noaa_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d272d8bc-2e2a-4937-bbef-1c4bcd029ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape before prep\n",
    "noaa_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17170293-b77d-4f9d-aa3f-03a2346193c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date columns from integer format (YYYYMMDD) to datetime\n",
    "noaa_prep['Begin Date'] = pd.to_datetime(noaa_prep['Begin Date'], format='%Y%m%d')\n",
    "noaa_prep['End Date'] = pd.to_datetime(noaa_prep['End Date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f296d-f36c-43fb-8b95-16df1d8133a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract year from Begin Date\n",
    "noaa_prep['year'] = noaa_prep['Begin Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af7c59-a7af-405e-8cf8-c119c5a0fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "noaa_prep[['Name', 'Begin Date', 'End Date', 'year']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed04c745-6d88-4ab4-85cd-a0682fd4c15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate NOAA data by year\n",
    "noaa_year = noaa_prep.groupby('year').agg({\n",
    "    'Name': 'count',  # Number of billion-dollar disasters\n",
    "    'Total CPI-Adjusted Cost (Millions of Dollars)': 'sum',  # Total cost\n",
    "    'Deaths': 'sum'  # Total deaths\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "noaa_year.columns = ['year', 'billion_dollar_disasters', 'total_disaster_cost_millions', 'total_deaths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0d9df8-e61a-497c-a465-1c479ab20cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the result\n",
    "noaa_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12dc01d-a4e9-4ae7-8510-ea784bf82c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save prepared NOAA data (national year-level)\n",
    "noaa_year.to_csv(DATA_RAW_DIR.parent / \"processed\" / \"noaa_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b97b94-447e-405f-a218-f2a9e376cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm save\n",
    "print(f\"NOAA data prepared: {noaa_year.shape[0]} years with {noaa_year.shape[1]} features\")\n",
    "print(\"Note: NOAA data is at national year-level, not county-level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb756055-99d1-469e-a833-57115ca37f85",
   "metadata": {},
   "source": [
    "#### BEA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6019be-3625-4d20-81c4-a2118ae52afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of BEA data for preparation\n",
    "bea_prep = bea_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c5b40-6983-40da-9a79-53ee98a2d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape before preparation\n",
    "bea_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f12ec-3b56-4aa6-a928-348c6629b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-digit county FIPS code\n",
    "bea_prep['county_fips'] = bea_prep['GeoFips'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd56c71b-31b1-4246-96c3-af2ad40e609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns and rename for clarity\n",
    "bea_prep = bea_prep[['county_fips', 'GeoName', 'TimePeriod', 'DataValue']].copy()\n",
    "bea_prep.columns = ['county_fips', 'county_name', 'year', 'gdp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8bf6e-5098-47ba-8288-3b88677c8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "bea_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500d061-dec0-4779-a975-99dbce30f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "bea_prep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71581b77-1c23-4542-b4f9-d01ac8640ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of years per county\n",
    "years_per_county = bea_prep.groupby('county_fips')['year'].count()\n",
    "years_per_county.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f567da-19a2-468e-80f3-4685a2fbe1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the year range\n",
    "print(f\"Year range: {bea_prep['year'].min()} to {bea_prep['year'].max()}\")\n",
    "print(f\"Number of years: {bea_prep['year'].nunique()}\")\n",
    "print(f\"Number of counties: {bea_prep['county_fips'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65639400-4880-4d04-abe3-ba5d4cf0e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by county and year to ensure proper ordering\n",
    "bea_prep = bea_prep.sort_values(['county_fips', 'year']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41dfa3d-9d03-45f4-9251-58fabb2f14dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate GDP 2 years later for each county-year\n",
    "bea_prep['gdp_plus_2'] = bea_prep.groupby('county_fips')['gdp'].shift(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098f314-093a-4c4f-a62b-05b9c712628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate recovery rate: (GDP_t+2 - GDP_t) / GDP_t\n",
    "bea_prep['recovery_rate'] = (bea_prep['gdp_plus_2'] - bea_prep['gdp']) / bea_prep['gdp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c480ef-6757-4893-aa1b-fd3be2b1ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "bea_prep[['county_fips', 'county_name', 'year', 'gdp', 'gdp_plus_2', 'recovery_rate']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bcb8d-0b41-446c-9832-fa6672104298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in recovery_rate\n",
    "bea_prep['recovery_rate'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754f790a-76c0-430a-b79a-cba9afac9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which years have missing recovery rates\n",
    "bea_prep[bea_prep['recovery_rate'].isnull()]['year'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861bebfa-62fe-4f39-8ff6-9fdea5f8f9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which counties have missing recovery rates in earlier years\n",
    "early_missing = bea_prep[(bea_prep['recovery_rate'].isnull()) & (bea_prep['year'] < 2022)]\n",
    "early_missing[['county_fips', 'county_name', 'year', 'gdp', 'gdp_plus_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3e31c-a7e7-451c-92a8-ee7bb0575296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where GDP is zero (non-existent counties in those years)\n",
    "bea_prep = bea_prep[bea_prep['gdp'] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fab2c9-1e8d-4a43-bd25-548f69267848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate recovery rate after filtering\n",
    "bea_prep = bea_prep.sort_values(['county_fips', 'year']).reset_index(drop=True)\n",
    "bea_prep['gdp_plus_2'] = bea_prep.groupby('county_fips')['gdp'].shift(-2)\n",
    "bea_prep['recovery_rate'] = (bea_prep['gdp_plus_2'] - bea_prep['gdp']) / bea_prep['gdp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d986e5-0ebb-4ae8-88ae-b1864fc2565b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values again\n",
    "print(f\"Total missing recovery rates: {bea_prep['recovery_rate'].isnull().sum()}\")\n",
    "bea_prep[bea_prep['recovery_rate'].isnull()]['year'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e066af6c-15bc-4346-b237-b4e9043a59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the remaining early missing values\n",
    "early_missing = bea_prep[(bea_prep['recovery_rate'].isnull()) & (bea_prep['year'] < 2022)]\n",
    "early_missing[['county_fips', 'county_name', 'year', 'gdp', 'gdp_plus_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6a12a-b00d-4be4-9f87-2522c2e2c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prepared BEA data (includes all rows)\n",
    "bea_prep.to_csv(DATA_RAW_DIR.parent / \"processed\" / \"bea_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd24c9-dd4a-446f-847c-6031bec567c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm save\n",
    "print(f\"BEA data prepared: {bea_prep.shape[0]:,} county-year observations with {bea_prep.shape[1]} features\")\n",
    "print(f\"Observations with valid recovery rates: {bea_prep['recovery_rate'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dd2022-0914-421c-a7cf-5849eaf6c72c",
   "metadata": {},
   "source": [
    "#### Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe7310-a37c-4923-9489-4217f457a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of Census data for preparation\n",
    "census_prep = census_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a0b02-1245-44c0-89b7-3163f9571496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape before preparation\n",
    "census_prep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf75ab-bbe3-4331-8570-ffdb58d76fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5-digit county FIPS code\n",
    "census_prep['county_fips'] = (census_prep['state'].astype(str).str.zfill(2) + \n",
    "                               census_prep['county'].astype(str).str.zfill(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07499ca0-58c0-4098-993a-f61a6bc0ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to be more readable\n",
    "census_prep = census_prep.rename(columns={\n",
    "    'B19013_001E': 'median_household_income',\n",
    "    'B17001_002E': 'pop_below_poverty',\n",
    "    'B17001_001E': 'total_pop_poverty_status',\n",
    "    'B23025_005E': 'unemployed',\n",
    "    'B23025_003E': 'in_labor_force',\n",
    "    'B15003_022E': 'bachelors_degree',\n",
    "    'B15003_023E': 'masters_degree',\n",
    "    'B15003_024E': 'professional_degree',\n",
    "    'B15003_025E': 'doctorate_degree',\n",
    "    'B15003_001E': 'total_pop_25plus',\n",
    "    'B25003_002E': 'owner_occupied_housing',\n",
    "    'B25003_001E': 'total_occupied_housing',\n",
    "    'NAME': 'county_name'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfda68-65d9-47cf-ab3e-bb04827f4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "census_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644c92f8-c17f-48f7-9c80-0f6f3feaab07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate derived socioeconomic rates\n",
    "census_prep['poverty_rate'] = (census_prep['pop_below_poverty'] / census_prep['total_pop_poverty_status']) * 100\n",
    "census_prep['unemployment_rate'] = (census_prep['unemployed'] / census_prep['in_labor_force']) * 100\n",
    "census_prep['college_degree_rate'] = ((census_prep['bachelors_degree'] + census_prep['masters_degree'] + \n",
    "                                       census_prep['professional_degree'] + census_prep['doctorate_degree']) / \n",
    "                                      census_prep['total_pop_25plus']) * 100\n",
    "census_prep['homeownership_rate'] = (census_prep['owner_occupied_housing'] / census_prep['total_occupied_housing']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142d6e0-277b-41a2-9d4b-e4aca35f2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle the suppressed value (-666666666) in median_household_income\n",
    "# Replace with NaN for proper handling\n",
    "census_prep['median_household_income'] = census_prep['median_household_income'].replace(-666666666, pd.NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05511e66-43e9-4151-ab94-dff06af980e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "census_prep[['county_fips', 'county_name', 'median_household_income', 'poverty_rate', \n",
    "             'unemployment_rate', 'college_degree_rate', 'homeownership_rate']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5b8e1-217b-4fa7-9191-0bf55b027d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final columns for modeling\n",
    "census_cols = [\n",
    "    'county_fips',\n",
    "    'county_name',\n",
    "    'median_household_income',\n",
    "    'poverty_rate',\n",
    "    'unemployment_rate',\n",
    "    'college_degree_rate',\n",
    "    'homeownership_rate'\n",
    "]\n",
    "\n",
    "census_prep = census_prep[census_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe61b13-59ac-4d9a-94cf-88320614dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "census_prep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e95dc78-756c-4dc0-a393-f675108fad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final columns for modeling\n",
    "census_cols = [\n",
    "    'county_fips',\n",
    "    'county_name',\n",
    "    'median_household_income',\n",
    "    'poverty_rate',\n",
    "    'unemployment_rate',\n",
    "    'college_degree_rate',\n",
    "    'homeownership_rate'\n",
    "]\n",
    "\n",
    "census_prep = census_prep[census_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ec5e4-e69f-4ce7-81f2-9fabb50fa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "census_prep.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e398b7-f1cd-465a-812e-8ef4134c66e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prepared Census data\n",
    "census_prep.to_csv(DATA_RAW_DIR.parent / \"processed\" / \"census_prepared.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560bb7b3-68bd-4bc4-a215-48d3d60c3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm save\n",
    "print(f\"Census data prepared: {census_prep.shape[0]:,} counties with {census_prep.shape[1]} features\")\n",
    "print(f\"Missing median_household_income: {census_prep['median_household_income'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008698e7-1be4-4715-acb1-351d5e82c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with BEA data (has target variable: recovery_rate)\n",
    "# Only keep rows where we can calculate recovery rate (years 2001-2021)\n",
    "modeling_data = bea_prep[bea_prep['recovery_rate'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813115f1-04d3-4a8a-a247-a3634a8e17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "print(f\"Starting with BEA data: {modeling_data.shape}\")\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d6ebc-b17b-4940-88ff-25e7aa66d066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge FEMA data (county-year level)\n",
    "modeling_data = modeling_data.merge(\n",
    "    fema_final,\n",
    "    on=['county_fips', 'year'],\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c62fd-5c32-47cd-a6ed-e91eb9742f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "print(f\"After merging FEMA: {modeling_data.shape}\")\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafba7c8-dd28-44c8-9967-9e0b4979935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 for counties with no disasters in that year\n",
    "fema_columns = modeling_data.columns[modeling_data.columns.get_loc('disaster_count'):]\n",
    "modeling_data[fema_columns] = modeling_data[fema_columns].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a8278e-16d0-44d9-bdeb-5bbbfc20fcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "print(f\"Shape after filling FEMA NaNs: {modeling_data.shape}\")\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83388b-704e-45a7-9971-ef83e97aba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Census data (county level - same values for all years)\n",
    "modeling_data = modeling_data.merge(\n",
    "    census_prep,\n",
    "    on='county_fips',\n",
    "    how='left',\n",
    "    suffixes=('', '_census')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce4f10-0a57-4efd-a8c9-c5da72c230ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "print(f\"After merging Census: {modeling_data.shape}\")\n",
    "# Check if there are any counties in BEA that aren't in Census\n",
    "print(f\"Counties with missing Census data: {modeling_data['median_household_income'].isnull().sum()}\")\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd7a2e-cdab-471f-a0a2-bbd6220b7f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the duplicate county_name_census column (keep the original)\n",
    "modeling_data = modeling_data.drop(columns=['county_name_census'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12318e1-601a-4591-a6bd-096294c90339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the final shape and columns\n",
    "print(f\"Final modeling dataset shape: {modeling_data.shape}\")\n",
    "print(f\"\\nColumns: {list(modeling_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe168df4-c128-4ffc-834d-54ab2405647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any remaining missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(modeling_data.isnull().sum()[modeling_data.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229e188-e8cd-48ba-99d7-5f5cf52a303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which counties are missing Census data\n",
    "missing_census = modeling_data[modeling_data['median_household_income'].isnull()]['county_fips'].unique()\n",
    "print(f\"Number of counties missing Census data: {len(missing_census)}\")\n",
    "print(f\"Sample missing counties:\")\n",
    "modeling_data[modeling_data['county_fips'].isin(missing_census[:5])][['county_fips', 'county_name', 'year']].drop_duplicates('county_fips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4499ed50-c0be-4069-a278-cf171095c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final merged modeling dataset\n",
    "modeling_data.to_csv(DATA_RAW_DIR.parent / \"processed\" / \"modeling_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe4c7c-c6da-4370-940b-621def13c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(f\"\\nFinal modeling dataset saved:\")\n",
    "print(f\"  Total observations: {modeling_data.shape[0]:,}\")\n",
    "print(f\"  Features: {modeling_data.shape[1]}\")\n",
    "print(f\"  Counties: {modeling_data['county_fips'].nunique()}\")\n",
    "print(f\"  Years: {modeling_data['year'].min()}-{modeling_data['year'].max()}\")\n",
    "print(f\"  Observations with complete data: {modeling_data.dropna().shape[0]:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5efa261-a953-44e7-9651-7690c332bd91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Preparation Summary\n",
    "\n",
    "#### Prepared Datasets\n",
    "\n",
    "**1. FEMA Data** (`fema_prepared.csv`)\n",
    "- 47,646 county-year observations\n",
    "- 34 features: disaster counts, assistance programs, incident type breakdowns\n",
    "- County-year level aggregation\n",
    "\n",
    "**2. NOAA Data** (`noaa_prepared.csv`)\n",
    "- 44 years (1980-2023)\n",
    "- 4 features: billion-dollar disaster counts, total costs, deaths\n",
    "- National year-level (not county-specific)\n",
    "\n",
    "**3. BEA Data** (`bea_prepared.csv`)\n",
    "- 71,587 county-year observations\n",
    "- 6 features including **target variable: recovery_rate**\n",
    "- Recovery rate = (GDP_t+2 - GDP_t) / GDP_t\n",
    "\n",
    "**4. Census Data** (`census_prepared.csv`)\n",
    "- 3,222 counties\n",
    "- 7 features: income, poverty rate, unemployment rate, education, homeownership\n",
    "- 2022 ACS 5-year estimates\n",
    "\n",
    "#### Final Modeling Dataset (`modeling_data.csv`)\n",
    "- **65,351 observations** (county-years from 2001-2021 with valid recovery rates)\n",
    "- **43 features** including:\n",
    "  - Target: recovery_rate\n",
    "  - Economic: gdp, gdp_plus_2\n",
    "  - Disasters: disaster_count, assistance programs, incident types (27 types)\n",
    "  - Socioeconomic: 5 Census-derived rates\n",
    "- **3,118 counties**\n",
    "- **64,624 complete observations** (98.9% completeness)\n",
    "- Missing data: 727 observations missing Census data (county reorganizations)\n",
    "\n",
    "**Next Steps:** Move to Section 5 (Shortlist Promising Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016e529-c33a-42b0-bd96-eeb8a0938911",
   "metadata": {},
   "source": [
    "## 5. Shortlist Promising Models\n",
    "Notes:\n",
    " - If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or random forests).\n",
    " - Once again, try to automate these steps as much as possible.\n",
    "1. Train many quick-and-dirty models from different categories (e.g., linear, naive Bayes, SVM, random forest, neural net, etc.) using standard parameters.\n",
    "2. Measure and compare their performance:\n",
    "    - For each model, use N-fold cross-validation and compute the mean and standard deviation of the performance measure on the N folds.\n",
    "3. Analyze the most significant variables for each algorithm.\n",
    "4. Analyze the types of errors the models make:\n",
    "    - What data would a human have used to avoid these errors?\n",
    "5. Perform a quick round of feature selection and engineering.\n",
    "6. Perform one or two more quick iterations of the five previous steps.\n",
    "7. Shortlist the top three to five most promising models, preferring models that make different types of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bc8d98-c157-4bd1-84ad-a2c5c6a3dad7",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc36703-7af2-4ad0-979d-5ce537a16d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared modeling data\n",
    "modeling_data = pd.read_csv(DATA_RAW_DIR.parent / \"processed\" / \"modeling_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702e542-640e-4a29-899b-f89a2848a2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape\n",
    "print(f\"Modeling data loaded: {modeling_data.shape}\")\n",
    "modeling_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9571354a-3d0d-4675-8b37-2a95422efd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "# Drop non-feature columns: county identifiers, intermediate calculations\n",
    "drop_cols = ['county_fips', 'county_name', 'year', 'gdp', 'gdp_plus_2', 'recovery_rate']\n",
    "\n",
    "X = modeling_data.drop(columns=drop_cols)\n",
    "y = modeling_data['recovery_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afd039-1c2b-48df-8a8c-fe4d1c816bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what we have\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns ({X.shape[1]} total):\")\n",
    "print(list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea2d03b-8082-4ae1-8bca-1589dba37064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in features\n",
    "print(\"Missing values in features:\")\n",
    "print(X.isnull().sum()[X.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e398e83-5dab-4a5c-8ee7-2bc3f533b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in target\n",
    "print(f\"\\nMissing values in target: {y.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5347b3-f047-4c2f-a0da-604e07f3de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "# Use random_state for reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496a690-deda-4ea2-8c7a-faf35da9e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the split\n",
    "print(f\"Training set: {X_train.shape[0]:,} observations\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bfdb4-9185-46b6-a467-88058a574a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in X\n",
    "print(\"Missing values in X:\")\n",
    "print(X.isnull().sum()[X.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca98a4-4a96-4297-80b7-9b605776672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check specifically in training set\n",
    "print(\"\\nMissing values in X_train:\")\n",
    "print(X_train.isnull().sum()[X_train.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa6650-d2fb-4ea0-b5d1-1ed362560604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create imputer (median strategy for robustness)\n",
    "imputer = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2338ff4-e457-4656-8203-589e8443eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data and transform both train and test\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b02c4b-b244-4c97-b315-d908c168e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DataFrames to keep column names\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "X_test_imputed = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "\n",
    "# Verify no missing values\n",
    "print(f\"Missing values in X_train after imputation: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in X_test after imputation: {X_test_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb42242d-5fd9-435d-ae34-56551427b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if X_train_imputed actually has NaN\n",
    "print(f\"Any NaN in X_train_imputed: {np.isnan(X_train_imputed).any()}\")\n",
    "print(f\"Any infinite values: {np.isinf(X_train_imputed).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c58776a-adba-4ddf-a0ba-1df3566f41a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data type\n",
    "print(f\"X_train_imputed type: {type(X_train_imputed)}\")\n",
    "print(f\"X_train_imputed dtype: {X_train_imputed.dtypes if hasattr(X_train_imputed, 'dtypes') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6afe6bb-e9ee-4f1a-8b53-50e25b071761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays explicitly\n",
    "X_train_np = X_train_imputed.to_numpy()\n",
    "X_test_np = X_test_imputed.to_numpy()\n",
    "\n",
    "# Check for NaN in numpy arrays\n",
    "print(f\"Any NaN in X_train_np: {np.isnan(X_train_np).any()}\")\n",
    "print(f\"Any NaN in y_train: {np.isnan(y_train).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d6b13-069a-43bd-9479-b5d482e458a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with numpy arrays\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_np, y_train)\n",
    "print(\"✓ Linear Regression model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31344065-10c3-42f6-bb6b-072fdf0ab02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on both train and test sets\n",
    "y_train_pred = linear_reg.predict(X_train_np)\n",
    "y_test_pred = linear_reg.predict(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c9230-ff9a-4c84-8e6a-04aa278f95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for training set\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105c735-8481-4834-a709-199fc618b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for test set\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  R² Score:  {train_r2:.4f}\")\n",
    "print(f\"  RMSE:      {train_rmse:.4f}\")\n",
    "print(f\"  MAE:       {train_mae:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R² Score:  {test_r2:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse:.4f}\")\n",
    "print(f\"  MAE:       {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da168a1-b27f-4d65-b3b6-afb26b2b5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_imputed.columns,\n",
    "    'coefficient': linear_reg.coef_\n",
    "})\n",
    "\n",
    "# Sort by absolute value of coefficient\n",
    "feature_importance['abs_coef'] = feature_importance['coefficient'].abs()\n",
    "feature_importance = feature_importance.sort_values('abs_coef', ascending=False)\n",
    "\n",
    "# Display top 10 most important features\n",
    "print(\"Top 10 Features by Coefficient Magnitude:\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_importance[['feature', 'coefficient']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b638b56-12c9-480a-b17b-8379e1b5f0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare full dataset (imputed)\n",
    "X_full_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6820b801-c246-421e-896a-43bf40c7c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on R², RMSE, and MAE\n",
    "cv_r2 = cross_val_score(linear_reg, X_full_imputed, y, cv=5, scoring='r2')\n",
    "cv_rmse = np.sqrt(-cross_val_score(linear_reg, X_full_imputed, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "cv_mae = -cross_val_score(linear_reg, X_full_imputed, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"Linear Regression - 5-Fold Cross-Validation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"R² Score:  {cv_r2.mean():.4f} (+/- {cv_r2.std():.4f})\")\n",
    "print(f\"RMSE:      {cv_rmse.mean():.4f} (+/- {cv_rmse.std():.4f})\")\n",
    "print(f\"MAE:       {cv_mae.mean():.4f} (+/- {cv_mae.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c1baf-44f7-4458-a2ad-772ba3633e4c",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "The cross-validation confirms Linear Regression performs very poorly (R² ≈ 0, essentially no better than predicting the mean). This is a clear baseline showing we need non-linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238c0ba-c4f0-4476-ab9d-9c91f6e92a3a",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a527eb-289c-481a-8d30-9b5f17916cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with reasonable defaults\n",
    "rf_reg = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all CPU cores\n",
    ")\n",
    "\n",
    "print(\"Training Random Forest (this may take a minute)...\")\n",
    "rf_reg.fit(X_train_np, y_train)\n",
    "print(\"✓ Random Forest model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b2092-e5c0-4cc1-a54b-d80ed11cb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_rf = rf_reg.predict(X_train_np)\n",
    "y_test_pred_rf = rf_reg.predict(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b15011-e7b3-4c9a-9967-e9ccf915d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_r2_rf = r2_score(y_train, y_train_pred_rf)\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
    "train_mae_rf = mean_absolute_error(y_train, y_train_pred_rf)\n",
    "\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "test_mae_rf = mean_absolute_error(y_test, y_test_pred_rf)\n",
    "\n",
    "# Display results\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  R² Score:  {train_r2_rf:.4f}\")\n",
    "print(f\"  RMSE:      {train_rmse_rf:.4f}\")\n",
    "print(f\"  MAE:       {train_mae_rf:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R² Score:  {test_r2_rf:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_rf:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143cce38-6d27-4b96-8b87-136389ef07b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation for Random Forest\n",
    "print(\"Running Random Forest cross-validation (this may take a few minutes)...\")\n",
    "\n",
    "cv_r2_rf = cross_val_score(rf_reg, X_full_imputed, y, cv=5, scoring='r2')\n",
    "cv_rmse_rf = np.sqrt(-cross_val_score(rf_reg, X_full_imputed, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "cv_mae_rf = -cross_val_score(rf_reg, X_full_imputed, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"\\nRandom Forest - 5-Fold Cross-Validation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"R² Score:  {cv_r2_rf.mean():.4f} (+/- {cv_r2_rf.std():.4f})\")\n",
    "print(f\"RMSE:      {cv_rmse_rf.mean():.4f} (+/- {cv_rmse_rf.std():.4f})\")\n",
    "print(f\"MAE:       {cv_mae_rf.mean():.4f} (+/- {cv_mae_rf.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79744853-110a-4799-aad1-0870c3252139",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "Random Forest also performs poorly in cross-validation (R² ≈ -0.07). The overfitting and poor generalization suggest we may need different hyperparameters or a different approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23313e9a-e4e7-4543-9189-e8e027d73d64",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8c391-d907-416b-9cfa-97e017f171ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Gradient Boosting\n",
    "gb_reg = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Gradient Boosting (this may take a minute)...\")\n",
    "gb_reg.fit(X_train_np, y_train)\n",
    "print(\"✓ Gradient Boosting model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c2195-c634-4827-8509-f68d91ba0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_gb = gb_reg.predict(X_train_np)\n",
    "y_test_pred_gb = gb_reg.predict(X_test_np)\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2_gb = r2_score(y_train, y_train_pred_gb)\n",
    "train_rmse_gb = np.sqrt(mean_squared_error(y_train, y_train_pred_gb))\n",
    "train_mae_gb = mean_absolute_error(y_train, y_train_pred_gb)\n",
    "\n",
    "test_r2_gb = r2_score(y_test, y_test_pred_gb)\n",
    "test_rmse_gb = np.sqrt(mean_squared_error(y_test, y_test_pred_gb))\n",
    "test_mae_gb = mean_absolute_error(y_test, y_test_pred_gb)\n",
    "\n",
    "# Display results\n",
    "print(\"Gradient Boosting Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  R² Score:  {train_r2_gb:.4f}\")\n",
    "print(f\"  RMSE:      {train_rmse_gb:.4f}\")\n",
    "print(f\"  MAE:       {train_mae_gb:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R² Score:  {test_r2_gb:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_gb:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777075d-740a-4577-ac16-25324d3948c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation for Gradient Boosting\n",
    "print(\"Running Gradient Boosting cross-validation (this may take a few minutes)...\")\n",
    "\n",
    "cv_r2_gb = cross_val_score(gb_reg, X_full_imputed, y, cv=5, scoring='r2')\n",
    "cv_rmse_gb = np.sqrt(-cross_val_score(gb_reg, X_full_imputed, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "cv_mae_gb = -cross_val_score(gb_reg, X_full_imputed, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"\\nGradient Boosting - 5-Fold Cross-Validation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"R² Score:  {cv_r2_gb.mean():.4f} (+/- {cv_r2_gb.std():.4f})\")\n",
    "print(f\"RMSE:      {cv_rmse_gb.mean():.4f} (+/- {cv_rmse_gb.std():.4f})\")\n",
    "print(f\"MAE:       {cv_mae_gb.mean():.4f} (+/- {cv_mae_gb.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e3db7-aa50-4016-a602-dffc542b39b7",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "Gradient Boosting also struggles in cross-validation (R² ≈ -0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679da22-1d88-4431-ad9e-dcfec4661469",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790545cb-b38a-4587-8c64-cc5ca3d5d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train LightGBM\n",
    "lgb_reg = LGBMRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1  # Suppress warnings\n",
    ")\n",
    "\n",
    "print(\"Training LightGBM (this may take a minute)...\")\n",
    "lgb_reg.fit(X_train_np, y_train)\n",
    "print(\"✓ LightGBM model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec3769-9650-48ee-acc7-59e739bf1b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using DataFrames\n",
    "y_train_pred_lgb = lgb_reg.predict(X_train_imputed)\n",
    "y_test_pred_lgb = lgb_reg.predict(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658c5904-cd21-4af4-aabc-45de88c3f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_r2_lgb = r2_score(y_train, y_train_pred_lgb)\n",
    "train_rmse_lgb = np.sqrt(mean_squared_error(y_train, y_train_pred_lgb))\n",
    "train_mae_lgb = mean_absolute_error(y_train, y_train_pred_lgb)\n",
    "\n",
    "test_r2_lgb = r2_score(y_test, y_test_pred_lgb)\n",
    "test_rmse_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_lgb))\n",
    "test_mae_lgb = mean_absolute_error(y_test, y_test_pred_lgb)\n",
    "\n",
    "# Display results\n",
    "print(\"LightGBM Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  R² Score:  {train_r2_lgb:.4f}\")\n",
    "print(f\"  RMSE:      {train_rmse_lgb:.4f}\")\n",
    "print(f\"  MAE:       {train_mae_lgb:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R² Score:  {test_r2_lgb:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_lgb:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cc420b-b814-4aec-ad8f-db354d6a0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation for LightGBM\n",
    "print(\"Running LightGBM cross-validation (this may take a few minutes)...\")\n",
    "\n",
    "cv_r2_lgb = cross_val_score(lgb_reg, X_full_imputed, y, cv=5, scoring='r2')\n",
    "cv_rmse_lgb = np.sqrt(-cross_val_score(lgb_reg, X_full_imputed, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "cv_mae_lgb = -cross_val_score(lgb_reg, X_full_imputed, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"\\nLightGBM - 5-Fold Cross-Validation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"R² Score:  {cv_r2_lgb.mean():.4f} (+/- {cv_r2_lgb.std():.4f})\")\n",
    "print(f\"RMSE:      {cv_rmse_lgb.mean():.4f} (+/- {cv_rmse_lgb.std():.4f})\")\n",
    "print(f\"MAE:       {cv_mae_lgb.mean():.4f} (+/- {cv_mae_lgb.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff7fc8-6a61-4d5c-8334-a2458562a64b",
   "metadata": {},
   "source": [
    "##### Findings\n",
    "LightGBM cross-validation shows R² ≈ 0, similar to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11654df0-f225-4043-8e47-3ab9870545f1",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddfb701-6c53-4c04-98ba-04ee226e5bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train XGBoost\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost (this may take a minute)...\")\n",
    "xgb_reg.fit(X_train_imputed, y_train)\n",
    "print(\"✓ XGBoost model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48d15d-fdc2-4baa-af24-526411e5622f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_xgb = xgb_reg.predict(X_train_imputed)\n",
    "y_test_pred_xgb = xgb_reg.predict(X_test_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dda493-0e07-41ca-99c8-b9861626727f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "train_r2_xgb = r2_score(y_train, y_train_pred_xgb)\n",
    "train_rmse_xgb = np.sqrt(mean_squared_error(y_train, y_train_pred_xgb))\n",
    "train_mae_xgb = mean_absolute_error(y_train, y_train_pred_xgb)\n",
    "\n",
    "test_r2_xgb = r2_score(y_test, y_test_pred_xgb)\n",
    "test_rmse_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))\n",
    "test_mae_xgb = mean_absolute_error(y_test, y_test_pred_xgb)\n",
    "\n",
    "# Display results\n",
    "print(\"XGBoost Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Set:\")\n",
    "print(f\"  R² Score:  {train_r2_xgb:.4f}\")\n",
    "print(f\"  RMSE:      {train_rmse_xgb:.4f}\")\n",
    "print(f\"  MAE:       {train_mae_xgb:.4f}\")\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"  R² Score:  {test_r2_xgb:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_xgb:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdb290-1d3d-4e04-9041-d2573274de77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation for XGBoost\n",
    "print(\"Running XGBoost cross-validation (this may take a few minutes)...\")\n",
    "\n",
    "# Convert X_full_imputed to DataFrame to avoid warnings\n",
    "X_full_df = pd.DataFrame(X_full_imputed, columns=X.columns)\n",
    "\n",
    "cv_r2_xgb = cross_val_score(xgb_reg, X_full_df, y, cv=5, scoring='r2')\n",
    "cv_rmse_xgb = np.sqrt(-cross_val_score(xgb_reg, X_full_df, y, cv=5, scoring='neg_mean_squared_error'))\n",
    "cv_mae_xgb = -cross_val_score(xgb_reg, X_full_df, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Display cross-validation results\n",
    "print(\"\\nXGBoost - 5-Fold Cross-Validation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"R² Score:  {cv_r2_xgb.mean():.4f} (+/- {cv_r2_xgb.std():.4f})\")\n",
    "print(f\"RMSE:      {cv_rmse_xgb.mean():.4f} (+/- {cv_rmse_xgb.std():.4f})\")\n",
    "print(f\"MAE:       {cv_mae_xgb.mean():.4f} (+/- {cv_mae_xgb.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9591a1f1-81c5-434c-b122-1b56044423e4",
   "metadata": {},
   "source": [
    "#### Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a53b17-f8b3-4a31-af9c-6617822a7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table of all models\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Random Forest', 'Gradient Boosting', 'LightGBM', 'XGBoost'],\n",
    "    'CV R²': [cv_r2.mean(), cv_r2_rf.mean(), cv_r2_gb.mean(), cv_r2_lgb.mean(), cv_r2_xgb.mean()],\n",
    "    'CV R² Std': [cv_r2.std(), cv_r2_rf.std(), cv_r2_gb.std(), cv_r2_lgb.std(), cv_r2_xgb.std()],\n",
    "    'CV RMSE': [cv_rmse.mean(), cv_rmse_rf.mean(), cv_rmse_gb.mean(), cv_rmse_lgb.mean(), cv_rmse_xgb.mean()],\n",
    "    'CV MAE': [cv_mae.mean(), cv_mae_rf.mean(), cv_mae_gb.mean(), cv_mae_lgb.mean(), cv_mae_xgb.mean()],\n",
    "    'Test R²': [test_r2, test_r2_rf, test_r2_gb, test_r2_lgb, test_r2_xgb],\n",
    "    'Test RMSE': [test_rmse, test_rmse_rf, test_rmse_gb, test_rmse_lgb, test_mae_xgb],\n",
    "    'Test MAE': [test_mae, test_mae_rf, test_mae_gb, test_mae_lgb, test_mae_xgb]\n",
    "})\n",
    "\n",
    "# Sort by CV R² (descending)\n",
    "results_comparison = results_comparison.sort_values('CV R²', ascending=False)\n",
    "\n",
    "print(\"\\nModel Comparison Summary (Sorted by CV R²):\")\n",
    "print(\"=\" * 100)\n",
    "print(results_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8dcc2-9106-4814-b18c-53bc507a2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from LightGBM\n",
    "feature_importance_lgb = pd.DataFrame({\n",
    "    'feature': X_train_imputed.columns,\n",
    "    'importance': lgb_reg.feature_importances_\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "feature_importance_lgb = feature_importance_lgb.sort_values('importance', ascending=False)\n",
    "\n",
    "# Display top 15 features\n",
    "print(\"LightGBM - Top 15 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_importance_lgb.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4f048-b8c1-41b7-ba1d-270adedbb2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 10 features\n",
    "ax = feature_importance_lgb.head(10).plot(x='feature', y='importance', kind='barh', figsize=(10, 6))\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title('LightGBM - Top 10 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c22167-55e5-4c29-b9a4-8e03062bf93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Gradient Boosting feature importance\n",
    "feature_importance_gb = pd.DataFrame({\n",
    "    'feature': X_train_imputed.columns,\n",
    "    'importance': gb_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nGradient Boosting - Top 15 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_importance_gb.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761cf4bd-3b43-4e0e-b446-1ac56c0daf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "feature_importance_xgb = pd.DataFrame({\n",
    "    'feature': X_train_imputed.columns,\n",
    "    'importance': xgb_reg.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nXGBoost - Top 15 Most Important Features:\")\n",
    "print(\"=\" * 60)\n",
    "print(feature_importance_xgb.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec71f5-149c-457b-b1a1-f63672a4e1dd",
   "metadata": {},
   "source": [
    "### Section 5 Summary: Model Shortlisting Results\n",
    "\n",
    "#### Model Performance Comparison (5-Fold Cross-Validation)\n",
    "\n",
    "All models showed poor predictive performance, with R² scores near zero:\n",
    "\n",
    "| Model | CV R² | CV RMSE | CV MAE |\n",
    "|-------|-------|---------|--------|\n",
    "| **LightGBM** | **-0.002** | **0.152** | **0.082** |\n",
    "| Linear Regression | -0.002 | 0.152 | 0.082 |\n",
    "| Gradient Boosting | -0.016 | 0.153 | 0.082 |\n",
    "| XGBoost | -0.017 | 0.153 | 0.082 |\n",
    "| Random Forest | -0.067 | 0.157 | 0.085 |\n",
    "\n",
    "**Key Finding:** GDP recovery rates are extremely difficult to predict using disaster and socioeconomic features alone. All models perform essentially at baseline (R² ≈ 0).\n",
    "\n",
    "#### Feature Importance Insights\n",
    "\n",
    "Across all tree-based models, the top predictors are:\n",
    "1. **Socioeconomic factors** (70-80% of importance): median income, poverty rate, unemployment rate, education, homeownership\n",
    "2. **Disaster types** (15-25% of importance): Severe Storm, Fire, Hurricane, Severe Ice Storm\n",
    "3. **Assistance programs** (5-10% of importance): PA, IA, HM programs\n",
    "\n",
    "**Interpretation:** Baseline economic conditions matter more than disaster characteristics for predicting recovery, but overall predictive power is very weak.\n",
    "\n",
    "#### Models Selected for Fine-Tuning (Section 6)\n",
    "\n",
    "1. **LightGBM** - Best CV performance\n",
    "2. **Gradient Boosting** - Second best, different implementation\n",
    "3. **XGBoost** - Close performance, worth tuning\n",
    "\n",
    "Linear Regression and Random Forest eliminated due to poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea22d9-3adb-447e-a9a3-fe33bff8e998",
   "metadata": {},
   "source": [
    "## 6. Fine-Tune the System\n",
    "Notes:\n",
    " - You will want to use as much data as possible for this step, especially as you move toward the end of fine-tuning.\n",
    " - As always, automate what you can.\n",
    "1. Fine-tune the hyperparameters using cross-validation:\n",
    "    - Treat your data transformation choices as hyperparameters, especially when you are not sure about them (e.g., if you’re not sure whether to replace missing values with zeros or with the median value, or to just drop the rows).\n",
    "    - Unless there are very few hyperparameter values to explore, prefer random search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, as described by Jasper Snoek et al.1).\n",
    "2. Try ensemble methods. Combining your best models will often produce better performance than running them individually.\n",
    "3. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c4f24-854f-4f3a-9467-f4815e378e76",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c92fd-e185-4079-adb3-d43cfac06581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space for LightGBM\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10, -1],  # -1 means no limit\n",
    "    'num_leaves': [15, 31, 63, 127],\n",
    "    'min_child_samples': [5, 10, 20, 30],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter search space defined for LightGBM\")\n",
    "print(f\"Total possible combinations: {4*4*5*4*4*3*3} = {4*4*5*4*4*3*3:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0157d910-ff6f-4b68-bb47-c26a2484495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RandomizedSearchCV for LightGBM\n",
    "# We'll sample 50 combinations (manageable but thorough)\n",
    "lgb_random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_reg,\n",
    "    param_distributions=lgb_param_grid,\n",
    "    n_iter=50,  # Number of parameter combinations to try\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    scoring='r2',  # Optimize for R²\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"RandomizedSearchCV configured:\")\n",
    "print(f\"  Testing 50 random combinations\")\n",
    "print(f\"  5-fold cross-validation\")\n",
    "print(f\"  Optimizing for R² score\")\n",
    "print(\"\\nThis will take several minutes to run...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abe2e3-6748-46c9-8ee0-de97f22a27c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit RandomizedSearchCV\n",
    "# Convert to DataFrame to avoid warnings\n",
    "X_full_df = pd.DataFrame(X_full_imputed, columns=X.columns)\n",
    "\n",
    "lgb_random_search.fit(X_full_df, y)\n",
    "\n",
    "print(\"✓ Hyperparameter search complete!\")\n",
    "print(f\"\\nBest parameters found:\")\n",
    "print(lgb_random_search.best_params_)\n",
    "print(f\"\\nBest cross-validation R² score: {lgb_random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dc591e-7cd6-48e2-b3f9-7b34904adb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_lgb = lgb_random_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_best_lgb = best_lgb.predict(X_test_imputed)\n",
    "\n",
    "test_r2_best_lgb = r2_score(y_test, y_test_pred_best_lgb)\n",
    "test_rmse_best_lgb = np.sqrt(mean_squared_error(y_test, y_test_pred_best_lgb))\n",
    "test_mae_best_lgb = mean_absolute_error(y_test, y_test_pred_best_lgb)\n",
    "\n",
    "print(\"Tuned LightGBM - Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  R² Score:  {test_r2_best_lgb:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_best_lgb:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_best_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e0117-c59c-43d0-8aa2-ed6da7c05e13",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e441b5e-ece0-4a15-a717-c85e82373366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space for Gradient Boosting\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5, 10],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV for Gradient Boosting\n",
    "gb_random_search = RandomizedSearchCV(\n",
    "    estimator=gb_reg,\n",
    "    param_distributions=gb_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"RandomizedSearchCV configured for Gradient Boosting\")\n",
    "print(\"Running hyperparameter search (this will take several minutes)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d688b1ca-c417-4676-b48f-f6897ba57f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit RandomizedSearchCV for Gradient Boosting\n",
    "gb_random_search.fit(X_full_imputed, y)\n",
    "\n",
    "print(\"✓ Hyperparameter search complete!\")\n",
    "print(f\"\\nBest parameters found:\")\n",
    "print(gb_random_search.best_params_)\n",
    "print(f\"\\nBest cross-validation R² score: {gb_random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7e46d2-8250-42ef-bd28-a368395c33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_gb = gb_random_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_best_gb = best_gb.predict(X_test_imputed)\n",
    "\n",
    "test_r2_best_gb = r2_score(y_test, y_test_pred_best_gb)\n",
    "test_rmse_best_gb = np.sqrt(mean_squared_error(y_test, y_test_pred_best_gb))\n",
    "test_mae_best_gb = mean_absolute_error(y_test, y_test_pred_best_gb)\n",
    "\n",
    "print(\"Tuned Gradient Boosting - Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  R² Score:  {test_r2_best_gb:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_best_gb:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_best_gb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d0d87-42f5-4d1f-9ac3-b60b056ad4d8",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d047f-66cf-45fc-9106-6b0841292188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space for XGBoost\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV for XGBoost\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_reg,\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='r2',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"RandomizedSearchCV configured for XGBoost\")\n",
    "print(\"Running hyperparameter search (this will take several minutes)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a81bf7-7451-42d4-b18f-f6071412752e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit RandomizedSearchCV for XGBoost\n",
    "xgb_random_search.fit(X_full_df, y)\n",
    "\n",
    "print(\"✓ Hyperparameter search complete!\")\n",
    "print(f\"\\nBest parameters found:\")\n",
    "print(xgb_random_search.best_params_)\n",
    "print(f\"\\nBest cross-validation R² score: {xgb_random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcb58b-94f8-4a2b-9c11-1e9852192530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_xgb = xgb_random_search.best_estimator_\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred_best_xgb = best_xgb.predict(X_test_imputed)\n",
    "\n",
    "test_r2_best_xgb = r2_score(y_test, y_test_pred_best_xgb)\n",
    "test_rmse_best_xgb = np.sqrt(mean_squared_error(y_test, y_test_pred_best_xgb))\n",
    "test_mae_best_xgb = mean_absolute_error(y_test, y_test_pred_best_xgb)\n",
    "\n",
    "print(\"Tuned XGBoost - Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  R² Score:  {test_r2_best_xgb:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_best_xgb:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_best_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc319b01-591f-433d-b2d2-345459391875",
   "metadata": {},
   "source": [
    "### Ensemble Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c3978-5297-4ef5-934a-84edc5d3750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tuned models\n",
    "tuned_comparison = pd.DataFrame({\n",
    "    'Model': ['Tuned LightGBM', 'Tuned Gradient Boosting', 'Tuned XGBoost'],\n",
    "    'CV R²': [lgb_random_search.best_score_, gb_random_search.best_score_, xgb_random_search.best_score_],\n",
    "    'Test R²': [test_r2_best_lgb, test_r2_best_gb, test_r2_best_xgb],\n",
    "    'Test RMSE': [test_rmse_best_lgb, test_rmse_best_gb, test_rmse_best_xgb],\n",
    "    'Test MAE': [test_mae_best_lgb, test_mae_best_gb, test_mae_best_xgb]\n",
    "})\n",
    "\n",
    "tuned_comparison = tuned_comparison.sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(\"\\nTuned Model Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "print(tuned_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681f0161-cca3-4c4e-b725-02828c20dca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble by averaging predictions from all three tuned models\n",
    "y_test_pred_ensemble = (y_test_pred_best_lgb + y_test_pred_best_gb + y_test_pred_best_xgb) / 3\n",
    "\n",
    "# Evaluate ensemble\n",
    "test_r2_ensemble = r2_score(y_test, y_test_pred_ensemble)\n",
    "test_rmse_ensemble = np.sqrt(mean_squared_error(y_test, y_test_pred_ensemble))\n",
    "test_mae_ensemble = mean_absolute_error(y_test, y_test_pred_ensemble)\n",
    "\n",
    "print(\"Ensemble (Average of 3 Models) - Test Set Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  R² Score:  {test_r2_ensemble:.4f}\")\n",
    "print(f\"  RMSE:      {test_rmse_ensemble:.4f}\")\n",
    "print(f\"  MAE:       {test_mae_ensemble:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a42145-7dd6-4b89-89f9-4a7ea867f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison including ensemble\n",
    "final_comparison = pd.DataFrame({\n",
    "    'Model': ['Tuned Gradient Boosting', 'Ensemble (3 Models)', 'Tuned LightGBM', 'Tuned XGBoost'],\n",
    "    'Test R²': [test_r2_best_gb, test_r2_ensemble, test_r2_best_lgb, test_r2_best_xgb],\n",
    "    'Test RMSE': [test_rmse_best_gb, test_rmse_ensemble, test_rmse_best_lgb, test_rmse_best_xgb],\n",
    "    'Test MAE': [test_mae_best_gb, test_mae_ensemble, test_mae_best_lgb, test_mae_best_xgb]\n",
    "})\n",
    "\n",
    "final_comparison = final_comparison.sort_values('Test R²', ascending=False)\n",
    "\n",
    "print(\"\\nFinal Model Comparison (Test Set):\")\n",
    "print(\"=\" * 70)\n",
    "print(final_comparison.to_string(index=False))\n",
    "print(\"\\n✓ Winner: Tuned Gradient Boosting\")\n",
    "print(f\"  Final Test R²: {test_r2_best_gb:.4f}\")\n",
    "print(f\"  Explains ~5.7% of variance in GDP recovery rates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e60aff-1618-4e3f-81be-c5abf2dde159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from best Gradient Boosting model\n",
    "final_feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_imputed.columns,\n",
    "    'importance': best_gb.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFinal Model (Tuned Gradient Boosting) - Top 15 Features:\")\n",
    "print(\"=\" * 60)\n",
    "print(final_feature_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed38ab-dc94-4406-87f9-dd549656f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 10 features\n",
    "ax = final_feature_importance.head(10).plot(x='feature', y='importance', kind='barh', figsize=(10, 6))\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title('Final Model - Top 10 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e44fa2-0360-47dc-89b3-8d8c69bbcbbe",
   "metadata": {},
   "source": [
    "### Section 6 Summary: Fine-Tuning Results\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "Used RandomizedSearchCV with 50 iterations and 5-fold cross-validation to tune three models:\n",
    "\n",
    "**1. Tuned LightGBM**\n",
    "- Best params: learning_rate=0.01, n_estimators=500, max_depth=3, num_leaves=127\n",
    "- CV R²: 0.0043 (improved from -0.002)\n",
    "- Test R²: 0.0390\n",
    "\n",
    "**2. Tuned Gradient Boosting** ⭐ **WINNER**\n",
    "- Best params: learning_rate=0.01, n_estimators=200, max_depth=7, subsample=0.8\n",
    "- CV R²: 0.0025 (improved from -0.016)\n",
    "- Test R²: **0.0572** (improved from 0.035)\n",
    "\n",
    "**3. Tuned XGBoost**\n",
    "- Best params: learning_rate=0.01, n_estimators=100, max_depth=3, subsample=0.8\n",
    "- CV R²: 0.0013 (improved from -0.017)\n",
    "- Test R²: 0.0191\n",
    "\n",
    "#### Ensemble Method\n",
    "\n",
    "Simple averaging ensemble of all three models:\n",
    "- Test R²: 0.0399 (did not outperform Gradient Boosting alone)\n",
    "\n",
    "#### Final Model Performance\n",
    "\n",
    "**Tuned Gradient Boosting (Best Model):**\n",
    "- **Test R²: 0.0572** - Explains 5.7% of variance in GDP recovery rates\n",
    "- **Test RMSE: 0.1414**\n",
    "- **Test MAE: 0.0793**\n",
    "\n",
    "#### Top Predictors (Feature Importance)\n",
    "\n",
    "1. College degree rate (19.7%)\n",
    "2. Median household income (18.2%)\n",
    "3. Unemployment rate (17.3%)\n",
    "4. Poverty rate (16.2%)\n",
    "5. Homeownership rate (12.8%)\n",
    "\n",
    "**Key Finding:** Baseline socioeconomic conditions account for 84% of feature importance, while disaster characteristics (frequency, type, assistance) account for only 16%.\n",
    "\n",
    "#### Model Limitations\n",
    "\n",
    "Despite hyperparameter tuning, predictive power remains very low (R² = 0.057). This suggests:\n",
    "- GDP recovery is driven by factors not captured in current features\n",
    "- High inherent noise/randomness in economic recovery patterns\n",
    "- May need additional data: pre-disaster economic trends, infrastructure quality, industry composition, policy responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8860e43-93d6-4c87-bda3-ee6b2f5d0a6a",
   "metadata": {},
   "source": [
    "## 7. Present your Solution\n",
    "1. Document what you have done.\n",
    "2. Create a nice presentation:\n",
    "    - Make sure you highlight the big picture first.\n",
    "3. Explain why your solution achieves the business objective.\n",
    "4. Don’t forget to present interesting points you noticed along the way:\n",
    "    - Describe what worked and what did not.\n",
    "    - List your assumptions and your system’s limitations.\n",
    "5. Ensure your key findings are communicated through beautiful visualizations or easy-to-remember statements (e.g., “the median income is the number-one predictor of housing prices”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9235995-251c-4c2e-91c5-8bbd9b97b627",
   "metadata": {},
   "source": [
    "## 8. Launch!\n",
    "1. Get your solution ready for production (plug into production data inputs, write unit tests, etc.).\n",
    "2. Write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops:\n",
    "    - Beware of slow degradation: models tend to “rot” as data evolves.\n",
    "    - Measuring performance may require a human pipeline (e.g., via a crowdsourcing service).\n",
    "    - Also monitor your inputs’ quality (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale). This is particularly important for online learning systems.\n",
    "3. Retrain your models on a regular basis on fresh data (automate as much as possible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751ed45-81ab-43f2-adc1-b17534672e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
